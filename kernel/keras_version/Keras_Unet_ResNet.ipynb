{
  "cells": [
    {
      "metadata": {
        "_uuid": "c8fc6c3e84f282d0e6f664caf9848297b6c62a16"
      },
      "cell_type": "markdown",
      "source": "### U-net with simple Resnet Blocks v2, can get 0.80+\n        \n        \n#### update log- the 2nd run\n1.   pretrained part we use weighted-dice-bce-loss instead of binary loss \n2.  image augmentation (pad and  crop v3)\n     \nLimit the max epochs number to make the kernel finish in the limit of 6 hours, better score can be achived at more epochs "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy import ndimage\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage import filters\n\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras import optimizers\nfrom tqdm import tqdm_notebook\nfrom keras.losses import binary_crossentropy\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n\nfrom imgaug import augmenters as iaa\n\nimport time\nt_start = time.time()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c36d16775556c3e358edefab5710dc1541ccfc4"
      },
      "cell_type": "code",
      "source": "version = 10\nbasic_name = f'Unet_resnet_v{version}'\nsave_model_name = basic_name + '.model'\nsubmission_file = basic_name + '.csv'\n\nprint(save_model_name)\nprint(submission_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63c469280793719bf311d51e6ba2cdaea157d175"
      },
      "cell_type": "code",
      "source": "img_size_ori = 101\nimg_size_target = 101\n\ndef upsample(img):# not used\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    \ndef downsample(img):# not used\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a64babef03b9a0dbc94387a1dad54971c3e028d"
      },
      "cell_type": "code",
      "source": "# Loading of training/testing ids and depths\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\nlen(train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c3768717007fb5f087d3e01619f1a9f9a3beac"
      },
      "cell_type": "code",
      "source": "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f55103f7daad6f03ec874c643077fe686c31bee"
      },
      "cell_type": "code",
      "source": "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "010066dd50ef4fdfa7dabe2c946fd7491f9556fd"
      },
      "cell_type": "code",
      "source": "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ad5ac1576277fc54d768933c36efd1f9ff01acd"
      },
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f34c4263989a3d95af1e5922c1b7d2126655610"
      },
      "cell_type": "code",
      "source": "#Plotting the depth distributionsÂ¶\n\nsns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b71c1ee8a090f4896b90ac8931865a73e44e1b79"
      },
      "cell_type": "code",
      "source": "def is_africa_boundary(img):\n    if np.sum(img) != 0:\n        edges_h = filters.sobel_h(img) \n        if np.sum(edges_h) == 0:\n            return False\n        return True\n    return True",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "582e28ae4a769f6f6d519b8ec4d5afa0e4a222b0"
      },
      "cell_type": "code",
      "source": "images = np.array(train_df.images.tolist())\nmasks = np.array(train_df.masks.tolist())\n\nafrica_boundary = []\nfor mask in masks:\n    africa_boundary.append(is_africa_boundary(mask))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51981795f0dd6b8ca7abe4db367f48313b63811e"
      },
      "cell_type": "code",
      "source": "# Create train/validation split stratified by salt coverage\n\nids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fb577cdf27f365d4a912728c2a7654d0e60fac8"
      },
      "cell_type": "code",
      "source": "def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02967d71ee7f936254ab54acf2aa7c2e038a2b21"
      },
      "cell_type": "code",
      "source": "# Build model\ndef build_model(input_layer, start_neurons, DropoutRatio = 0.5):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n    output_layer =  Activation('sigmoid')(output_layer_noActi)\n    \n    return output_layer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bd5e479d3aa211bcb5fe32ce9c4d71cd012eefa"
      },
      "cell_type": "code",
      "source": "def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b599462393e8f6a2f4daab809bc6b91fe42e0a0"
      },
      "cell_type": "code",
      "source": "def focal_loss(gamma=2, alpha=0.75):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e44f8385530720dc99e2b92b1cfcd014775bf780"
      },
      "cell_type": "code",
      "source": "def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "072ab621d38cc93d26998f391357cb6efc791600"
      },
      "cell_type": "code",
      "source": "#Data augmentation\nx_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\nprint(x_train.shape)\nprint(y_valid.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7af5f7d18ffedce5d6cf7e897e838fee340ac401"
      },
      "cell_type": "code",
      "source": "def form_double_batch(X, y, batch_size):\n    idx = np.random.randint(0, X.shape[0], int(batch_size))\n    return X[idx], y[idx]\n\ndef double_batch_generator(X, y, batch_size):\n    st = lambda aug: iaa.Sometimes(0.5, aug)\n    affine_seq = iaa.Sequential([\n        st(iaa.CropAndPad(percent=(0.8,1.2), pad_mode=\"reflect\")),\n        # General\n        st(iaa.Affine(\n                scale={\"x\": (1, 1.2), \"y\": (1, 1.2)}, # scale images to 80-120% of their size, individually per axis\n            )),\n        # Deformations\n        #iaa.Sometimes(0.3, iaa.PiecewiseAffine(scale=(0.04, 0.08))),\n        #iaa.Sometimes(0.3, iaa.PerspectiveTransform(scale=(0.05, 0.1))),\n    ], random_order=False)\n\n    intensity_seq = iaa.Sequential([\n        iaa.Invert(0.3),\n        iaa.Sometimes(0.3, iaa.ContrastNormalization((0.5, 1.5))),\n        iaa.OneOf([\n            iaa.Noop(),\n            iaa.Sequential([\n                iaa.OneOf([\n                    iaa.Add((-10, 10)),\n                    iaa.AddElementwise((-10, 10)),\n                    iaa.Multiply((0.95, 1.05)),\n                    iaa.MultiplyElementwise((0.95, 1.05)),\n                ]),\n            ]),\n            iaa.OneOf([\n                iaa.GaussianBlur(sigma=(0.0, 1.0)),\n                iaa.AverageBlur(k=(2, 5)),\n                #iaa.MedianBlur(k=(3, 5))\n            ])\n        ])\n    ], random_order=False)\n\n\n    while True:\n        x_batch, y_batch = form_double_batch(X, y, batch_size)\n        affine_seq_det = affine_seq.to_deterministic()\n        new_x_batch = affine_seq_det.augment_images(x_batch*255)\n        new_x_batch = intensity_seq.augment_images(new_x_batch)\n        new_y_batch = affine_seq_det.augment_images(y_batch*255)\n        yield new_x_batch/255, new_y_batch/255",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30622932f68888e895a9b8cac91810a1bb3c5e75"
      },
      "cell_type": "code",
      "source": "# model\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.5)\n\nmodel1 = Model(input_layer, output_layer)\n\nc = optimizers.adam(lr = 0.01)\nmodel1.compile(loss=bce_dice_loss, optimizer=c, metrics=[my_iou_metric])\n\nmodel1.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41699081be465c14e193ffad4fd00bd56840f156",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "early_stopping = EarlyStopping(monitor='val_my_iou_metric', mode = 'max',patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n\nepochs = 100\nbatch_size = 64\nhistory = model1.fit_generator(double_batch_generator(x_train, y_train, batch_size),\n                              validation_data=[x_valid, y_valid],\n                              steps_per_epoch=200,\n                              epochs=epochs,\n                              callbacks=[early_stopping, model_checkpoint, reduce_lr])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c89c406884ee54bee2c57aff51b116c157553ae9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "t_finish = time.time()\nprint(f\"Kernel run time = {(t_finish-t_start)/3600} hours\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0607489d361741ffae881d4a7b00b1cf744fd278"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}