{
  "cells": [
    {
      "metadata": {
        "_uuid": "54643e32b4132d15df20d55f2b9ee09efc2c51a8"
      },
      "cell_type": "markdown",
      "source": "# Libraries"
    },
    {
      "metadata": {
        "_uuid": "b15761a022f76bb50db1bb55c21b59a3f1c88261",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nimport shutil\n\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy import ndimage\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage import filters\n\nfrom imgaug import augmenters as iaa\n\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torchvision.transforms import ToTensor, ToPILImage, Normalize, Compose\n\nimport PIL\n\nfrom datetime import datetime\nimport json\nimport gc\n\nimport time",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "99e7207d6aec7d921e4b8ad4f30d310d2346216a"
      },
      "cell_type": "markdown",
      "source": "# Global variable"
    },
    {
      "metadata": {
        "_uuid": "73cc76d8506e96b912600406fa5ceeb47083978a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAIN_IMG_PATH = \"../input/tgs-salt-identification-challenge/train/images/\"\nTEST_IMG_PATH = \"../input/tgs-salt-identification-challenge/test/images/\"\nDEPTH_PATH = \"../input/tgs-salt-identification-challenge/depths.csv\"\nTRAIN_MASK_PATH = \"../input/tgs-salt-identification-challenge/train/masks/\"\nTRAIN_INFO_PATH = \"../input/tgs-salt-identification-challenge/train.csv\"\nFILENAME = \"checkpoint.pth.tar\"\nBEST_FILENAME = \"model_best.pth.tar\"\n\n# basic parameters\nIMG_ORI_SIZE = 101\nIMG_TAR_SIZE = 128\nSCALE = 1\n\n# Model parameters\nSTART_NEURONS = 16\nDROPOUT_RATIO = 0.5\n\nINPUT_CHANNEL = 3\n\nMODEL1_ADAM_LR = 0.01\nMODEL1_EPOCHS = 100\nMODEL1_BATCH_SIZE = 64\nMODEL1_STEPS_PER_EPOCH_TRAIN = 200\nMODEL1_LOSS = 'binary_crossentropy'\n\nMODEL2_ADAM_LR = 0.01\nMODEL2_EPOCHS = 100\nMODEL2_BATCH_SIZE = 64\nMODEL2_STEPS_PER_EPOCH_TRAIN = 200\nMODEL2_LOSS = 'lovasz_loss'\n\n# ReduceLROnPlateau parameters\nMODEL1_REDUCE_FACTOR = 0.5\nMODEL1_REDUCE_PATIENT = 5\n\nMODEL2_REDUCE_FACTOR = 0.5\nMODEL2_REDUCE_PATIENT = 5\n\n# DICE_BCE_LOSS Parameters\nBCE_WEIGHT = 1\nDICE_WEIGHT = 0\n\n# Augmentation Parmeters\nAUG = True\nFIT_METHOD = 'resize'\nPAD_METHOD = 'edge'\n\nversion = 1\n\n# Name\nBASIC_NAME = f'Unet_resnet_v{version}'\nSAVE_MODEL_NAME = BASIC_NAME + '.model'\nSUBMISSION_NAME = BASIC_NAME + '.csv'\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ba265639935d6bc1518daed79744747896651d4"
      },
      "cell_type": "code",
      "source": "# encoding=utf8\nimport cv2\nimport numpy as np\nfrom torch import nn\nfrom functools import reduce\n\n################################################################################\n# related functions & loss functions\n################################################################################\n\n\ndef upsample(img):\n    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n        return img\n    return cv2.resize(img, (IMG_TAR_SIZE, IMG_TAR_SIZE))\n\n\ndef downsample(img):\n    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n        return img\n    return cv2.resize(img, (IMG_ORI_SIZE, IMG_ORI_SIZE))\n\n\ndef add_depth_channels(image_array, depth):\n    image_array[:,:,1] = depth\n    image_array[:,:,2] = image_array[:,:,0] * image_array[:,:,1]\n    return image_array\n\n\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n        \ndef write_event(log, step: int, **data):\n    data['step'] = step\n    data['dt'] = datetime.now().isoformat()\n    log.write(json.dumps(data, sort_keys=True, cls=MyEncoder))\n    log.write('\\n')\n    log.flush()\n\n    \ndef get_variable(x):\n    \"\"\" Converts tensors to cuda, if available. \"\"\"\n    if torch.cuda.is_available():\n        return x.cuda()\n    return x\n\n\ndef get_numpy(x):\n    \"\"\" Get numpy array for both cuda and not. \"\"\"\n    if torch.cuda.is_available():\n        return x.cpu().data.numpy()\n    return x.data.numpy()\n  \n    \ndef iou_numpy(outputs, labels):\n    SMOOTH = 1e-6\n    labels = labels.squeeze(1)\n    outputs = outputs.squeeze(1)\n    \n    intersection = (outputs & labels).sum((1, 2))\n    union = (outputs | labels).sum((1, 2))\n    \n    iou = (intersection + SMOOTH) / (union + SMOOTH)\n    \n    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n    \n    return thresholded.mean()\n\n\ndef my_iou_metric(label, pred):\n    return iou_numpy(pred > 0.5, label>0.5)\n\n\ndef my_iou_metric_2(label, pred):\n    return iou_numpy(pred > 0, label>0.5)\n\ndef my_iou_metric_pad(label, pred):\n    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n    return iou_numpy(pred[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>0.5,label[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>0.5)\n\n    \ndef save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, 'model_best.pth.tar')\n        \n\nclass EarlyStopping(object):\n    def __init__(self, mode='min', min_delta=0, patience=10):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta)\n\n        if patience == 0:\n            self.is_better = lambda a, b: True\n\n    def step(self, metrics):\n        if self.best is None:\n            self.best = metrics\n            return False\n\n        if np.isnan(metrics):\n            return True\n\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            return True\n\n        return False\n\n    def _init_is_better(self, mode, min_delta):\n        if mode not in {'min', 'max'}:\n            raise ValueError('mode ' + mode + ' is unknown!')\n        if mode == 'min':\n            self.is_better = lambda a, best: a < best - min_delta\n        if mode == 'max':\n            self.is_better = lambda a, best: a > best + min_delta\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aa113e7f4cabb11b9a14afc2151e96ff664ef7ed",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class DiceLoss(nn.Module):\n    def __init__(self, smooth=0, eps=1e-7):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n        self.eps = eps\n\n    def forward(self, output, target):\n        return 1 - (2 * torch.sum(output * target) + self.smooth) / (\n            torch.sum(output) + torch.sum(target) + self.smooth + self.eps)\n\n\nclass Dice_Bce_Loss(nn.Module):\n    def __init__(self, smooth=0, eps=1e-7, dice_weight=0.2, \n                 dice_loss=None, bce_weight=0.9, bce_loss=None):\n        super(Dice_Bce_Loss, self).__init__()\n        self.smooth = smooth\n        self.eps = eps\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n        self.bce_loss = bce_loss\n        self.dice_loss = dice_loss\n        \n        if self.bce_loss is None:\n            self.bce_loss = nn.BCELoss()\n        if self.dice_loss is None:\n            self.dice_loss = DiceLoss(smooth, eps)\n            \n    def forward(self, output, target):\n        return self.dice_weight * self.dice_loss(output, target) + self.bce_weight * self.bce_loss(output, target)\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n        \n\n\"\"\"\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n\"\"\"\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.cumsum(0)\n    union = gts + (1 - gt_sorted).cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels - 1.\n    errors = (1. - logits * signs)\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), grad)\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    nanmean compatible with generators.\n    \"\"\"\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ef0e5853d0c2cc72dbf8a9fe90720a610a8dcccd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import imgaug as ia\nfrom imgaug import augmenters as iaa\nimport numpy as np\n\nia.seed(2018)\n\ndef _standardize(img):\n    return (img - img.map(np.mean)) / img.map(np.std)\n\naffine_seq = iaa.Sequential([\n    # General\n    iaa.SomeOf((1, 2),\n               [iaa.Fliplr(0.5),\n                iaa.Affine(rotate=(-5, 5),\n                           translate_percent={\"x\": (-0.05, 0.05)},\n                           mode=PAD_METHOD),\n                # iaa.CropAndPad(percent=((0.0, 0.0), (0.05, 0.0), (0.0, 0.0), (0.05, 0.0)))\n                ]),\n    # Deformations\n    iaa.Sometimes(0.3, iaa.PiecewiseAffine(scale=(0.04, 0.08))),\n    iaa.Sometimes(0.3, iaa.PerspectiveTransform(scale=(0.05, 0.1))),\n], random_order=True)\n\nintensity_seq = iaa.Sequential([\n    iaa.Invert(0.3),\n    iaa.Sometimes(0.3, iaa.ContrastNormalization((0.5, 1.5))),\n    iaa.OneOf([\n        iaa.Noop(),\n        iaa.Sequential([\n            iaa.OneOf([\n                iaa.Add((-10, 10)),\n                iaa.AddElementwise((-10, 10)),\n                iaa.Multiply((0.95, 1.05)),\n                iaa.MultiplyElementwise((0.95, 1.05)),\n            ]),\n        ]),\n        iaa.OneOf([\n            iaa.GaussianBlur(sigma=(0.0, 1.0)),\n            iaa.AverageBlur(k=(2, 5)),\n            #iaa.MedianBlur(k=(3, 5))\n        ])\n    ])\n], random_order=False)\n\ntta_intensity_seq = iaa.Sequential([\n    iaa.Noop()\n], random_order=False)\n\ndef resize_pad_seq(pad_method, pad_size):\n    seq = iaa.Sequential([\n        affine_seq,\n        iaa.Scale({'height': IMG_ORI_SIZE*SCALE, 'width': IMG_ORI_SIZE*SCALE}),\n        iaa.Pad(px=(pad_size, pad_size, pad_size+1, pad_size+1), pad_mode=pad_method, keep_size=False)\n    ], random_order=False)\n    return seq\n\ndef resize_pad_seq_eval(pad_method, pad_size):\n    seq = iaa.Sequential([\n        iaa.Scale({'height': IMG_ORI_SIZE*SCALE, 'width': IMG_ORI_SIZE*SCALE}),\n        iaa.Pad(px=(pad_size, pad_size, pad_size+1, pad_size+1), pad_mode=pad_method, keep_size=False)\n    ], random_order=False)\n    return seq\n\ndef resize_seq():\n    seq = iaa.Sequential([\n        affine_seq,\n        iaa.Scale({'height': IMG_TAR_SIZE, 'width': IMG_TAR_SIZE})\n    ], random_order=False)\n    return seq\n\ndef resize_seq_eval():\n    seq = iaa.Sequential([\n        iaa.Scale({'height': IMG_TAR_SIZE, 'width': IMG_TAR_SIZE})\n    ], random_order=False)\n    return seq",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1de194accd07b923821cc2f665f35d4502420869",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# encoding=utf8\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nimport cv2\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\n## convert salt coverage to class\ndef _cov_to_class(mask):\n    border = 10\n    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n\n    cover = (mask>0.5).sum()\n    if cover < 8:\n        return 0 # empty\n    if cover == ((mask*outer) > 0.5).sum():\n        return 1 #border\n    if np.all(mask==mask[0]):\n        return 2 #vertical\n\n    percentage = cover/(101*101)\n    if percentage < 0.15:\n        return 3\n    elif percentage < 0.25:\n        return 4\n    elif percentage < 0.50:\n        return 5\n    elif percentage < 0.75:\n        return 6\n    else:\n        return 7\n\n## used to load data from data files\nclass my_DataLoader:\n\n    def __init__(self, train=False, test=False, Kflod=False, test_size=0.2, num_flod=5):\n        self.test = test\n        self.train = train\n        self.Kflod = Kflod\n        self.test_size = test_size\n        \n        if self.Kflod:\n            self.num_flod = num_flod\n        else:\n            self.num_flod = 0\n            \n        train_df, self.test_df = self._load_depth()\n        \n        if self.train:\n            self._load_image_mask(train_df)\n            train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(IMG_ORI_SIZE, 2)\n            train_df[\"coverage_class\"] = train_df.masks.map(_cov_to_class)\n            self.x_train, self.x_valid, self.y_train, self.y_valid = self._get_train_test_split(train_df)\n            train_df = None\n            \n        if self.test:\n            test_df['images'] = self._load_image_test(self.test_df)\n            self.x_test = np.array(self.test_df.images.tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1)\n\n    @staticmethod\n    def _load_image_mask(train_df):\n        # load image data & mask data\n        train_df['images'] = [np.array(cv2.imread(TRAIN_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n        train_df['masks'] = [np.array(cv2.imread(TRAIN_MASK_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n        # Normalize image vectors\n        #train_df['images'] /= 255\n        #train_df['masks'] /= 255\n        \n    @staticmethod\n    def _load_image_test(test_df):\n        return [np.array(cv2.imread(TEST_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(test_df.index)]\n\n    @staticmethod\n    def _load_depth():\n        train_df = pd.read_csv(TRAIN_INFO_PATH, index_col=\"id\", usecols=[0])\n        depths_df = pd.read_csv(DEPTH_PATH, index_col=\"id\")\n        depths_df['z'] = depths_df['z'].astype('float')\n        train_df = train_df.join(depths_df)\n        test_df = depths_df[~depths_df.index.isin(train_df.index)]\n        return train_df, test_df\n\n    ## get train & validation split stratified by salt coverage\n    @staticmethod\n    def _get_train_test_split(train_df):\n        x_train, x_valid, y_train, y_valid = train_test_split(\n            np.array(train_df.images.tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1),\n            np.array(train_df.masks.tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1),\n            test_size=0.2, stratify=train_df.coverage_class, random_state=1234)\n        return x_train, x_valid, y_train, y_valid\n\n    def get_train(self):\n        return self.x_train, self.y_train\n    \n    def get_valid(self):\n        return self.x_valid, self.y_valid\n\n    def get_test_x(self):\n        return self.x_test\n\n    def get_test_df(self):\n        return self.test_df\n    \nclass ShipDataset(Dataset):\n    def __init__(self, data, transform=None, mode='train'):\n        if mode == 'train' or mode == 'valid':\n            self.x = data[0]\n            self.y = data[1]\n        elif mode == 'test':\n            self.data = data\n        else:\n            raise RuntimeError('MODE_ERROR')\n        self.transform = transform\n        self.mode = mode\n        self.pad_method = PAD_METHOD\n        self.pad_size = (IMG_TAR_SIZE-IMG_ORI_SIZE)//2\n        \n        if FIT_METHOD == 'resize_pad':\n            self.aug_func_eval = partial(resize_pad_seq_eval, self.pad_method, self.pad_size)\n        elif FIT_METHOD == 'resize':\n            self.aug_func_eval = resize_seq_eval\n            \n        if AUG:\n            if FIT_METHOD == 'resize_pad':\n                self.aug_func = partial(resize_pad_seq, self.pad_method, self.pad_size)\n            elif FIT_METHOD == 'resize':\n                self.aug_func = resize_seq\n        \n        if INPUT_CHANNEL == 3:\n            self.depth = np.tile(np.linspace(0,1,IMG_TAR_SIZE),[IMG_TAR_SIZE,1]).T\n\n    def __len__(self):\n        if self.mode == 'train' or self.mode == 'valid':\n            return len(self.x)\n        elif self.mode == 'test':\n            return len(self.data)\n        else:\n            raise RuntimeError('MODE_ERROR')\n               \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            if AUG:\n                resize_seq_det = self.aug_func().to_deterministic()\n                new_x_batch = resize_seq_det.augment_image(self.x[idx])\n                new_x_batch = intensity_seq.augment_image(new_x_batch)/255\n                new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n            else:\n                resize_seq_det = self.aug_func_eval().to_deterministic()\n                new_x_batch = resize_seq_det.augment_image(self.x[idx])/255\n                new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n            if INPUT_CHANNEL == 3:\n                new_x_batch = np.tile(new_x_batch,(1,1,3))\n                new_x_batch = add_depth_channels(new_x_batch, self.depth)\n            return torch.from_numpy(new_x_batch), torch.from_numpy(new_y_batch)\n        elif self.mode == 'valid':\n            resize_seq_det = self.aug_func_eval().to_deterministic()\n            new_x_batch = resize_seq_det.augment_image(self.x[idx])/255\n            new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n            if INPUT_CHANNEL == 3:\n                new_x_batch = np.tile(new_x_batch,(1,1,3))\n                new_x_batch = add_depth_channels(new_x_batch, self.depth)\n            return torch.from_numpy(new_x_batch), torch.from_numpy(new_y_batch)\n        elif self.mode == 'test':\n            resize_seq_det = self.aug_func_eval()\n            test_data = resize_seq_det.augment_image(self.data[idx])/255\n            if INPUT_CHANNEL == 3:\n                test_data = np.tile(test_data,(1,1,3))\n                new_x_batch = add_depth_channels(test_data, self.depth)\n            return torch.from_numpy(test_data)\n        else:\n            raise RuntimeError('MODE_ERROR')\n            \ndef make_loader(data, batch_size, num_workers=4, shuffle=False, transform=None, mode='train'):\n        return DataLoader(\n            dataset=ShipDataset(data, transform=transform, mode=mode),\n            shuffle=shuffle,\n            num_workers = num_workers,\n            batch_size = batch_size,\n            pin_memory=torch.cuda.is_available()\n        )",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47a024a662f85e6cbe936b1bce96d1c471533ddc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dl = my_DataLoader(train=True)\nx_train, y_train = dl.get_train()\nx_valid, y_valid = dl.get_valid()\n\n#Data augmentation\n#x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n#y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c4f66dfe6f24e19946d5909b479540b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fa3c4fb2b174a5194cd6849f0ca142d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "bbc578c9bfb6b93b501b74b721a456c0a60ef0f0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_loader = make_loader((x_train, y_train), batch_size=16, shuffle=True)\nvalid_loader = make_loader((x_valid, y_valid), num_workers=0, batch_size=64, mode='valid')",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "438c676e325d9eea868ee669225fe6bae0eeb769",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom torchvision import models\nimport torchvision\n\n\"\"\"\nThis script has been taken (and modified) from :\nhttps://github.com/ternaus/TernausNet\n@ARTICLE{arXiv:1801.05746,\n         author = {V. Iglovikov and A. Shvets},\n          title = {TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation},\n        journal = {ArXiv e-prints},\n         eprint = {1801.05746}, \n           year = 2018\n        }\n\"\"\"\n\n\nclass ConvBnRelu(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),\n                                  nn.BatchNorm2d(out_channels),\n                                  nn.ReLU(inplace=True)\n                                  )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass NoOperation(nn.Module):\n    def forward(self, x):\n        return x\n\n\nclass DecoderBlockV2(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlockV2, self).__init__()\n        self.is_deconv = is_deconv\n\n        self.deconv = nn.Sequential(\n            ConvBnRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n        self.conv = ConvBnRelu(in_channels, out_channels)\n\n    def forward(self, x):\n        if self.is_deconv:\n            x = self.deconv(x)\n        else:\n            x = self.conv(x)\n            x = F.interpolate(x, scale_factor=2, mode='bilinear')\n        return x\n\n\nclass UNetResNet(nn.Module):\n\n    def __init__(self, encoder_depth, num_classes, num_filters=32, dropout_2d=0.2,\n                 pretrained=False, is_deconv=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        if encoder_depth == 34:\n            self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n            bottom_channel_nr = 512\n        elif encoder_depth == 101:\n            self.encoder = torchvision.models.resnet101(pretrained=pretrained)\n            bottom_channel_nr = 2048\n        elif encoder_depth == 152:\n            self.encoder = torchvision.models.resnet152(pretrained=pretrained)\n            bottom_channel_nr = 2048\n        else:\n            raise NotImplementedError('only 34, 101, 152 version of Resnet are implemented')\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.input_adjust = nn.Sequential(self.encoder.conv1,\n                                          self.encoder.bn1,\n                                          self.encoder.relu)\n\n        self.conv1 = self.encoder.layer1\n        self.conv2 = self.encoder.layer2\n        self.conv3 = self.encoder.layer3\n        self.conv4 = self.encoder.layer4\n\n        self.dec4 = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)\n        self.dec3 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8,\n                                   is_deconv)\n        self.dec2 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2,\n                                   is_deconv)\n        self.dec1 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2,\n                                   is_deconv)\n        self.final = nn.Conv2d(num_filters * 2 * 2, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        input_adjust = self.input_adjust(x)\n        conv1 = self.conv1(input_adjust)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        center = self.conv4(conv3)\n        dec4 = self.dec4(center)\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = F.dropout2d(self.dec1(torch.cat([dec2, conv1], 1)), p=self.dropout_2d)\n        return self.final(dec1)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1e5972b534849e0cec3405de9ba678904411afe",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch.optim as optim\nfrom torch.autograd import Variable\n\nres_unet_noactiv = UNetResNet(34, 1, num_filters=16, dropout_2d=0.5, pretrained=False, is_deconv=False).double()\ncriterion = lovasz_hinge\n\nif torch.cuda.is_available():\n    res_unet_noactiv = res_unet_noactiv.cuda()\n    #criterion = criterion.cuda()\n\n#optimizer= Adam(res_unet.parameters(), lr=0.01)\n\nresume_path = '../input/shanlins-s-pretrained-res-unet-v2/model_best.pth.tar'\nif os.path.isfile(resume_path):\n    checkpoint = torch.load(resume_path)\n    #start_epoch = checkpoint['epoch']\n    best_iou = checkpoint['best_iou']\n    res_unet_noactiv.load_state_dict(checkpoint['state_dict'])\n    #optimizer.load_state_dict(checkpoint['optimizer'])\n\noptimizer= Adam(res_unet_noactiv.parameters(), lr=0.005)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66be681ece2c0b85c8c0db265979c57cd7097569",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def validation(model: nn.Module, criterion, valid_loader):\n    model.eval()\n    losses = []\n    iou = []\n    with torch.no_grad():\n        for inputs, targets in valid_loader:\n            inputs = inputs.permute(0,3,1,2).cuda()\n            targets = targets.permute(0,3,1,2).cuda()\n            outputs = model(inputs)\n            loss = criterion(torch.squeeze(outputs,1), torch.squeeze(targets,1))\n            losses += [loss.item()]\n            iou += [my_iou_metric_2(get_numpy(targets), get_numpy(outputs))]\n\n        valid_loss = np.mean(losses)  # type: float\n\n        valid_iou = np.mean(iou)\n\n        metrics = {'val_loss': valid_loss, 'val_iou': valid_iou}\n    return metrics\n\ndef train(model, criterion, train_loader, optimizer, epoch, scheduler,\n          report_each=10, valid_iou=0, fold=None):\n        model.train()\n        random.seed()\n        scheduler.step(valid_iou)\n        losses = []\n        ious = []\n        tl = train_loader\n        \n        try:\n            mean_loss = 0\n            mean_iou = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs = inputs.permute(0,3,1,2).cuda()\n                targets = targets.permute(0,3,1,2).cuda()\n                outputs = model(inputs)\n                loss = criterion(torch.squeeze(outputs,1), torch.squeeze(targets,1))\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                losses += [loss.item()]\n                ious += [my_iou_metric_2(get_numpy(targets), get_numpy(outputs))]\n                mean_loss = np.mean(losses[-report_each:])\n                mean_iou = np.mean(ious[-report_each:])\n\n                if i % report_each == 0:\n                    print('Epoch: [{0}][{1}/{2}]\\t'\n                          'Loss {loss:.4f} ({loss_avg:.4f})\\t'\n                          'IOU {iou:.3f} ({iou_avg:.3f})'.format(\n                           epoch, i, len(tl), loss=losses[-1], loss_avg=mean_loss, iou=ious[-1], iou_avg=mean_iou))\n\n            metrics = {'train_loss': mean_loss, 'train_iou': mean_iou}\n            return metrics\n        \n        except KeyboardInterrupt:\n            print('Ctrl+C, saving snapshot')\n            print('done.')\n            return",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ea6b2cc9a581db9b2d156bf5671ac58a9e4a0e8",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "scheduler = ReduceLROnPlateau(optimizer, mode='max', verbose=True, patience=3, factor=0.5, min_lr=0.0001)\n#model = nn.DataParallel(model, device_ids=None)\n\nn_epochs = 50\nstart_epoch = 0\nreport_each = 10\nvalid_losses = []\nvalid_ious = []\ntrain_losses = []\ntrain_ious = []\nvalid_iou = 0\nvalid_loss = 0\nfor epoch in range(start_epoch, n_epochs + 1):\n    \n    train_metrics = train(res_unet_noactiv, criterion, train_loader, optimizer, \n                          epoch, scheduler, report_each, valid_iou)\n    valid_metrics = validation(res_unet_noactiv, criterion, valid_loader)\n\n    train_loss = train_metrics['train_loss']\n    train_iou = train_metrics['train_iou']\n    train_losses += [train_loss]\n    train_ious += [train_iou]\n    valid_loss = valid_metrics['val_loss']\n    valid_iou = valid_metrics['val_iou']\n    valid_losses += [valid_loss]\n    valid_ious += [valid_iou]\n    print('Epoch: [{0}][Validation]\\t' \n          'Val_Loss: {val_loss:.5f}\\t' \n          'Val_IOU: {val_iou:.5f}'.format(epoch, val_loss=valid_loss, val_iou=valid_iou))\n    is_best = best_iou < valid_iou\n    best_iou = max(valid_iou, best_iou)\n    save_checkpoint({\n        'epoch': epoch + 1,\n        'state_dict': res_unet_noactiv.state_dict(),\n        'best_iou': best_iou,\n        'optimizer': optimizer.state_dict(),\n    }, is_best)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Epoch: [0][0/200]\tLoss 0.7393 (0.7393)\tIOU 0.600 (0.600)\nEpoch: [0][10/200]\tLoss 0.6533 (0.7669)\tIOU 0.625 (0.627)\nEpoch: [0][20/200]\tLoss 0.4078 (0.5815)\tIOU 0.769 (0.692)\nEpoch: [0][30/200]\tLoss 0.4868 (0.5153)\tIOU 0.731 (0.739)\nEpoch: [0][40/200]\tLoss 0.9392 (0.6117)\tIOU 0.475 (0.679)\nEpoch: [0][50/200]\tLoss 0.5799 (0.4421)\tIOU 0.694 (0.761)\nEpoch: [0][60/200]\tLoss 0.8312 (0.5702)\tIOU 0.594 (0.694)\nEpoch: [0][70/200]\tLoss 0.5817 (0.6207)\tIOU 0.688 (0.657)\nEpoch: [0][80/200]\tLoss 0.5079 (0.5251)\tIOU 0.700 (0.719)\nEpoch: [0][90/200]\tLoss 1.0257 (0.6411)\tIOU 0.487 (0.647)\nEpoch: [0][100/200]\tLoss 0.3114 (0.5189)\tIOU 0.819 (0.715)\nEpoch: [0][110/200]\tLoss 0.7863 (0.4788)\tIOU 0.562 (0.736)\nEpoch: [0][120/200]\tLoss 0.3892 (0.4856)\tIOU 0.825 (0.732)\nEpoch: [0][130/200]\tLoss 0.5927 (0.6414)\tIOU 0.600 (0.623)\nEpoch: [0][140/200]\tLoss 0.7865 (0.5630)\tIOU 0.550 (0.685)\nEpoch: [0][150/200]\tLoss 0.5526 (0.4989)\tIOU 0.662 (0.715)\nEpoch: [0][160/200]\tLoss 0.6389 (0.5150)\tIOU 0.700 (0.720)\nEpoch: [0][170/200]\tLoss 0.6452 (0.4963)\tIOU 0.675 (0.732)\nEpoch: [0][180/200]\tLoss 0.3410 (0.4698)\tIOU 0.800 (0.736)\nEpoch: [0][190/200]\tLoss 0.6294 (0.5453)\tIOU 0.700 (0.710)\nEpoch: [0][Validation]\tVal_Loss: 0.49867\tVal_IOU: 0.72440\nEpoch: [1][0/200]\tLoss 0.6684 (0.6684)\tIOU 0.619 (0.619)\nEpoch: [1][10/200]\tLoss 0.7198 (0.6255)\tIOU 0.587 (0.672)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Process Process-10:\nProcess Process-9:\nProcess Process-11:\nProcess Process-12:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"<ipython-input-6-b36125634b02>\", line 157, in __getitem__\n    new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 323, in augment_image\n    return self.augment_images([image], hooks=hooks)[0]\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 431, in augment_images\n    hooks=hooks\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 1522, in _augment_images\n    hooks=hooks\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 431, in augment_images\n    hooks=hooks\n  File \"/opt/conda/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 1510, in _augment_images\n    for index in random_state.permutation(len(self)):\nKeyboardInterrupt\n  File \"mtrand.pyx\", line 4904, in mtrand.RandomState.permutation\n  File \"mtrand.pyx\", line 4823, in mtrand.RandomState.shuffle\n  File \"/opt/conda/lib/python3.6/site-packages/numpy/core/_internal.py\", line 247, in __init__\n    def __init__(self, array, ptr=None):\nKeyboardInterrupt\n",
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid 100) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aca475e7e051>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(l, ignore_nan, empty)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aca475e7e051>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m         loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n\u001b[0;32m---> 99\u001b[0;31m                           for log, lab in zip(logits, labels))\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aca475e7e051>\u001b[0m in \u001b[0;36mlovasz_hinge_flat\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0merrors_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-dda1511823ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     train_metrics = train(res_unet_noactiv, criterion, train_loader, optimizer, \n\u001b[0;32m---> 16\u001b[0;31m                           epoch, scheduler, report_each, valid_iou)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mvalid_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_unet_noactiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bbc5f1f8a32c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, train_loader, optimizer, epoch, scheduler, report_each, valid_iou, fold)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aca475e7e051>\u001b[0m in \u001b[0;36mlovasz_hinge\u001b[0;34m(logits, labels, per_image, ignore)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mper_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n\u001b[0;32m---> 99\u001b[0;31m                           for log, lab in zip(logits, labels))\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlovasz_hinge_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflatten_binary_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aca475e7e051>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(l, ignore_nan, empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mempty\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 100) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace."
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "cc45d408a40354ab6609f7c41675eaaac979ac44",
        "trusted": false
      },
      "cell_type": "code",
      "source": "x_test = dl.get_test_x()\ntest_loader = make_loader(x_test, batch_size=64, mode='test')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08a07a21cbdaa905f726c1a3f09510d967f7e59e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "resume_path = BEST_FILENAME\nif os.path.isfile(resume_path):\n    checkpoint = torch.load(resume_path)\n    res_unet_noactiv.load_state_dict(checkpoint['state_dict'])\n    #optimizer.load_state_dict(checkpoint['optimizer'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c636982e1a4e7d34ed8c35717b747ed0d5cb3221",
        "trusted": false
      },
      "cell_type": "code",
      "source": "res_unet_noactiv.eval()\noutput_list = []\nwith torch.no_grad():\n    for i, (inputs, targets) in enumerate(valid_loader):\n        inputs = torch.FloatTensor(inputs).cuda()\n        outputs = res_unet_noactiv(inputs)\n        output_list += [get_numpy(outputs)]\n        if i % report_each == 0:\n            print('iteration: [{0}/{1}]'.format(i, len(valid_loader)))\noutput_valid = np.concatenate(output_list, 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b28aba45c492123696d58d80fa0617237eaeb356",
        "trusted": false
      },
      "cell_type": "code",
      "source": "label = y_valid.swapaxes(2,3).swapaxes(1,2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "266b6a8702c2c95d5d266d6657528059bebbb593",
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Scoring for last model, choose threshold by validation data \nthresholds_ori = np.linspace(0.3, 0.7, 31)\n# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\nthresholds = np.log(thresholds_ori/(1-thresholds_ori)) \nious = np.array([my_iou_metric_thre(label, output_valid, threshold) for threshold in tqdm_notebook(thresholds)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3652d8583024fc13240d77aebe5d9251d45f6f7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23286e49e518a5f4f4f921a52e0fb727229a3d64",
        "trusted": false
      },
      "cell_type": "code",
      "source": "res_unet_noactiv.eval()\noutput_list = []\nwith torch.no_grad():\n    for i, inputs in enumerate(test_loader):\n        inputs_1 = torch.FloatTensor(inputs)\n        inputs_2 = torch.flip(inputs_1, [3])\n        inputs_1, inputs_2 = inputs_1.cuda(), inputs_2.cuda()\n        outputs_1 = res_unet_noactiv(inputs_1)\n        outputs_2 = res_unet_noactiv(inputs_2)\n        outputs_2 = torch.flip(outputs_2, [3])\n        outputs = 0.5 * (outputs_1 + outputs_2)\n        output_list += [get_numpy(outputs)]\n        if i % report_each == 0:\n            print('iteration: [{0}/{1}]'.format(i, len(test_loader)))\noutput_test = np.concatenate(output_list, 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6adfb78ee8266fef50ec295f6441d37ee3ebfcab",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6751d1f5d2910b0d0397e8e551834b16c0d1e580",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_df = dl.get_test_df()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81520023c5e94d7922d1b74d3849a7647ddfc7ca",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pred_dict = {idx: rle_encode(output_test[i] > threshold_best) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df52d41ffce2d41d4d27ca345a947e3d2673bd17",
        "trusted": false
      },
      "cell_type": "code",
      "source": "submission_file = 'submission.csv'\nsub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv(submission_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc4e2e20af4ee28b0dfdccb1fe0ab1b57a04ff5d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}