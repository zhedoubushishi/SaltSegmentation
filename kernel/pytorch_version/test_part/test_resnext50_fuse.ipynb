{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54643e32b4132d15df20d55f2b9ee09efc2c51a8"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "b15761a022f76bb50db1bb55c21b59a3f1c88261"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from skimage import filters\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.backends.cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Normalize, Compose\n",
    "\n",
    "import PIL\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import time\n",
    "#t_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ec4d042f759143907062d0558e09600554e1d3a"
   },
   "source": [
    "# ResNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "cb2a2df265c41950ca3c921ed04d14a07ed57cd1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet code gently borrowed from\n",
    "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\"\"\"\n",
    "from __future__ import print_function, division, absolute_import\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n",
    "           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n",
    "\n",
    "pretrained_settings = {\n",
    "    'senet154': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnet50': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnet101': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnet152': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnext50_32x4d': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnext101_32x4d': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = int(math.floor(planes * (base_width / 64)) * groups)\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        block (nn.Module): Bottleneck class.\n",
    "            - For SENet154: SEBottleneck\n",
    "            - For SE-ResNet models: SEResNetBottleneck\n",
    "            - For SE-ResNeXt models:  SEResNeXtBottleneck\n",
    "        layers (list of ints): Number of residual blocks for 4 layers of the\n",
    "            network (layer1...layer4).\n",
    "        groups (int): Number of groups for the 3x3 convolution in each\n",
    "            bottleneck block.\n",
    "            - For SENet154: 64\n",
    "            - For SE-ResNet models: 1\n",
    "            - For SE-ResNeXt models:  32\n",
    "        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n",
    "            - For all models: 16\n",
    "        dropout_p (float or None): Drop probability for the Dropout layer.\n",
    "            If `None` the Dropout layer is not used.\n",
    "            - For SENet154: 0.2\n",
    "            - For SE-ResNet models: None\n",
    "            - For SE-ResNeXt models: None\n",
    "        inplanes (int):  Number of input channels for layer1.\n",
    "            - For SENet154: 128\n",
    "            - For SE-ResNet models: 64\n",
    "            - For SE-ResNeXt models: 64\n",
    "        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n",
    "            a single 7x7 convolution in layer0.\n",
    "            - For SENet154: True\n",
    "            - For SE-ResNet models: False\n",
    "            - For SE-ResNeXt models: False\n",
    "        downsample_kernel_size (int): Kernel size for downsampling convolutions\n",
    "            in layer2, layer3 and layer4.\n",
    "            - For SENet154: 3\n",
    "            - For SE-ResNet models: 1\n",
    "            - For SE-ResNeXt models: 1\n",
    "        downsample_padding (int): Padding for downsampling convolutions in\n",
    "            layer2, layer3 and layer4.\n",
    "            - For SENet154: 1\n",
    "            - For SE-ResNet models: 0\n",
    "            - For SE-ResNeXt models: 0\n",
    "        num_classes (int): Number of outputs in `last_linear` layer.\n",
    "            - For all models: 1000\n",
    "        \"\"\"\n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def initialize_pretrained_model(model, num_classes, settings):\n",
    "    assert num_classes == settings['num_classes'], \\\n",
    "        'num_classes should be {}, but is {}'.format(\n",
    "            settings['num_classes'], num_classes)\n",
    "    model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "    model.input_space = settings['input_space']\n",
    "    model.input_size = settings['input_size']\n",
    "    model.input_range = settings['input_range']\n",
    "    model.mean = settings['mean']\n",
    "    model.std = settings['std']\n",
    "\n",
    "\n",
    "def senet154(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n",
    "                  dropout_p=0.2, num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['senet154'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnet50'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnet101'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnet152'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99e7207d6aec7d921e4b8ad4f30d310d2346216a"
   },
   "source": [
    "# Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "73cc76d8506e96b912600406fa5ceeb47083978a"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMG_PATH = \"train/images/\"\n",
    "TEST_IMG_PATH = \"test/images/\"\n",
    "DEPTH_PATH = \"depths.csv\"\n",
    "TRAIN_MASK_PATH = \"train/masks/\"\n",
    "TRAIN_INFO_PATH = \"train.csv\"\n",
    "\n",
    "\n",
    "# basic parameters\n",
    "IMG_ORI_SIZE = 101\n",
    "IMG_TAR_SIZE = 128\n",
    "SCALE = 1\n",
    "\n",
    "# Model parameters\n",
    "START_NEURONS = 16\n",
    "DROPOUT_RATIO = 0.5\n",
    "\n",
    "LOAD_CHECKPONT = True\n",
    "\n",
    "INPUT_CHANNEL = 1\n",
    "\n",
    "MODEL1_ADAM_LR = 0.01\n",
    "MODEL1_EPOCHS = 100\n",
    "MODEL1_BATCH_SIZE = 64\n",
    "MODEL1_STEPS_PER_EPOCH_TRAIN = 200\n",
    "MODEL1_LOSS = 'binary_crossentropy'\n",
    "\n",
    "MODEL2_ADAM_LR = 0.01\n",
    "MODEL2_EPOCHS = 100\n",
    "MODEL2_BATCH_SIZE = 64\n",
    "MODEL2_STEPS_PER_EPOCH_TRAIN = 200\n",
    "MODEL2_LOSS = 'lovasz_loss'\n",
    "\n",
    "# ReduceLROnPlateau parameters\n",
    "MODEL1_REDUCE_FACTOR = 0.5\n",
    "MODEL1_REDUCE_PATIENT = 5\n",
    "\n",
    "MODEL2_REDUCE_FACTOR = 0.5\n",
    "MODEL2_REDUCE_PATIENT = 5\n",
    "\n",
    "# DICE_BCE_LOSS Parameters\n",
    "BCE_WEIGHT = 1\n",
    "DICE_WEIGHT = 0\n",
    "\n",
    "# Augmentation Parmeters\n",
    "AUG = True\n",
    "PAD_METHOD = 'edge'\n",
    "KFOLD = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "aa113e7f4cabb11b9a14afc2151e96ff664ef7ed"
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from functools import reduce\n",
    "\n",
    "################################################################################\n",
    "# related functions & loss functions\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def upsample(img):\n",
    "    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n",
    "        return img\n",
    "    return cv2.resize(img, (IMG_TAR_SIZE, IMG_TAR_SIZE))\n",
    "\n",
    "\n",
    "def downsample(img):\n",
    "    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n",
    "        return img\n",
    "    return cv2.resize(img, (IMG_ORI_SIZE, IMG_ORI_SIZE))\n",
    "\n",
    "\n",
    "def add_depth_channels(image_array, depth):\n",
    "    image_array[:,:,1] = depth\n",
    "    image_array[:,:,2] = image_array[:,:,0] * image_array[:,:,1]\n",
    "    return image_array\n",
    "\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "        \n",
    "def write_event(log, step: int, **data):\n",
    "    data['step'] = step\n",
    "    data['dt'] = datetime.now().isoformat()\n",
    "    log.write(json.dumps(data, sort_keys=True, cls=MyEncoder))\n",
    "    log.write('\\n')\n",
    "    log.flush()\n",
    "\n",
    "    \n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n",
    "def get_true_target(targets):\n",
    "    truth_image = targets.squeeze(3).sum(2).sum(1) > 0\n",
    "    return truth_image\n",
    "  \n",
    "\n",
    "def get_logits_outputs(outputs_image, outputs_pixel):\n",
    "    batch_size, C, H, W = outputs_pixel.shape\n",
    "    zero_mask = torch.zeros([batch_size, C, H, W], dtype=torch.float).to(device)\n",
    "    empty_label = outputs_image<0\n",
    "    outputs_pixel[empty_label] = zero_mask[empty_label]\n",
    "    return outputs_pixel\n",
    "\n",
    "\n",
    "def iou_numpy(outputs, labels):\n",
    "    SMOOTH = 1e-6\n",
    "    if len(outputs.shape) == 4:\n",
    "        outputs = outputs.squeeze(1)\n",
    "    if len(labels.shape) == 4:\n",
    "        labels = labels.squeeze(1) \n",
    "    \n",
    "    intersection = (outputs & labels).sum((1, 2))\n",
    "    union = (outputs | labels).sum((1, 2))\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n",
    "    \n",
    "    return thresholded.mean()\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return iou_numpy(pred > 0.5, label>0.5)\n",
    "\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return iou_numpy(pred > 0, label>0.5)\n",
    "\n",
    "\n",
    "def my_iou_metric_pad(label, pred):\n",
    "    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n",
    "    return iou_numpy(pred[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>0,label[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>0.5)\n",
    "\n",
    "\n",
    "def my_iou_metric_pad_thrs(label, pred, thrs):\n",
    "    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n",
    "    return iou_numpy(pred[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>thrs,label[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]>0.5)\n",
    "\n",
    "\n",
    "def my_iou_metric_resize_thrs(label, pred, thrs):\n",
    "    return iou_numpy(pred>thrs,label>0.5)\n",
    "\n",
    "\n",
    "def my_iou_metric_fuse_thrs(label, pred_pad, pred_resize, thrs):\n",
    "    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n",
    "    label = label[:,:,pad_size:-pad_size-1,pad_size+1:-pad_size]\n",
    "    pred_pad = pred_pad[:,0,pad_size:-pad_size-1,pad_size+1:-pad_size]\n",
    "    pred_resize = pred_resize.squeeze(1)\n",
    "    pred_resize = np.array([downsample(pred_resize_i) for pred_resize_i in pred_resize])\n",
    "    pred = (pred_pad+pred_resize)/2\n",
    "    return iou_numpy(pred>thrs,label>0.5)\n",
    "    \n",
    "\n",
    "def recover_size_pad(test_image, thre):\n",
    "    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n",
    "    recover_logits = downsample(test_image[pad_size:-pad_size-1,pad_size+1:-pad_size])\n",
    "    recover_binary = np.round(recover_logits > thre)\n",
    "    return recover_binary\n",
    "\n",
    "\n",
    "def recover_size(test_image, thre):\n",
    "    recover_logits = downsample(test_image)\n",
    "    recover_binary = np.round(recover_logits > thre)\n",
    "    return recover_binary\n",
    "\n",
    "\n",
    "def recover_size_pad_resize(test_image_pad, test_image_resize, thre):\n",
    "    pad_size = (IMG_TAR_SIZE-SCALE*IMG_ORI_SIZE)//2\n",
    "    recover_logits_pad = downsample(test_image_pad[pad_size:-pad_size-1,pad_size+1:-pad_size])\n",
    "    recover_logits_resize = downsample(test_image_resize)\n",
    "    recover_logits = (recover_logits_pad+recover_logits_resize)/2\n",
    "    recover_binary = np.round(recover_logits > thre)\n",
    "    return recover_binary\n",
    "    \n",
    "    \n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    check_filename = 'checkpoint_{}_resize.pth.tar'.format(filename)\n",
    "    torch.save(state, check_filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(check_filename, 'model_best_{}_resize.pth.tar'.format(filename))\n",
    "        \n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min':\n",
    "            self.is_better = lambda a, best: a < best - min_delta\n",
    "        if mode == 'max':\n",
    "            self.is_better = lambda a, best: a > best + min_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cd2ab81ce751b2b59dddaa43897b7ca7ca8db5b3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingLR_with_Restart(_LRScheduler):\n",
    "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. The original pytorch\n",
    "    implementation only implements the cosine annealing part of SGDR,\n",
    "    I added my own implementation of the restarts part.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        T_mult (float): Increase T_max by a factor of T_mult\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        model (pytorch model): The model to save.\n",
    "        out_dir (str): Directory to save snapshots\n",
    "        take_snapshot (bool): Whether to save snapshots at every restart\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, T_mult, model, out_dir, take_snapshot, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.Te = self.T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.current_epoch = last_epoch\n",
    "\n",
    "        self.model = model\n",
    "        self.out_dir = out_dir\n",
    "        self.take_snapshot = take_snapshot\n",
    "\n",
    "        self.lr_history = []\n",
    "\n",
    "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
    "                   (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
    "                   for base_lr in self.base_lrs]\n",
    "\n",
    "        self.lr_history.append(new_lrs)\n",
    "        return new_lrs\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        self.current_epoch += 1\n",
    "\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        ## restart\n",
    "        if self.current_epoch == self.Te:\n",
    "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
    "\n",
    "            if self.take_snapshot:\n",
    "                torch.save({\n",
    "                    'epoch': self.T_max,\n",
    "                    'state_dict': self.model.state_dict()\n",
    "                }, self.out_dir + \"Weight/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
    "\n",
    "            ## reset epochs since the last reset\n",
    "            self.current_epoch = 0\n",
    "\n",
    "            ## reset the next goal\n",
    "            self.Te = int(self.Te * self.T_mult)\n",
    "            self.T_max = self.T_max + self.Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "951cbf65518e980d4d9cc3595458a9b366374f58"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse\n",
    "    \n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=0, eps=1e-7):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output = torch.sigmoid(output)\n",
    "        return 1 - (2 * torch.sum(output * target) + self.smooth) / (torch.sum(output) + torch.sum(target) + self.smooth + self.eps)\n",
    "\n",
    "\n",
    "class Dice_Bce_Loss(nn.Module):\n",
    "    def __init__(self, smooth=0, eps=1e-7, dice_weight=0.5, \n",
    "                 dice_loss=None, bce_weight=0.9, bce_loss=None):\n",
    "        super(Dice_Bce_Loss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.bce_loss = bce_loss\n",
    "        self.dice_loss = dice_loss\n",
    "        \n",
    "        if self.bce_loss is None:\n",
    "            self.bce_loss = F.binary_cross_entropy_with_logits\n",
    "        if self.dice_loss is None:\n",
    "            self.dice_loss = DiceLoss(smooth, eps)\n",
    "            \n",
    "        self.activation = torch.sigmoid\n",
    "            \n",
    "    def forward(self, output, target):\n",
    "        output = self.activation(output)\n",
    "        return self.dice_weight * self.dice_loss(output, target) + self.bce_weight * self.bce_loss(output, target)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * signs)\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu(errors_sorted)+1, grad)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS ---------------------------\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "\n",
    "class BCE_Lovaz_Loss(nn.Module):\n",
    "    def __init__(self, per_image=True, ignore=None):\n",
    "        super(BCE_Lovaz_Loss, self).__init__()\n",
    "        self.per_image = per_image\n",
    "        self.ignore = ignore\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        loss_1 = lovasz_hinge(logits.squeeze(1), targets.squeeze(1), self.per_image, self.ignore)\n",
    "        loss_2 = nn.BCEWithLogitsLoss()(logits, targets)\n",
    "        loss = loss_1 + loss_2\n",
    "        return loss\n",
    "    \n",
    "class Lovaz_Loss(nn.Module):\n",
    "    def __init__(self, per_image=True, ignore=None):\n",
    "        super(Lovaz_Loss, self).__init__()\n",
    "        self.per_image = per_image\n",
    "        self.ignore = ignore\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        loss = lovasz_hinge(logits.squeeze(1), targets.squeeze(1), self.per_image, self.ignore)\n",
    "        return loss\n",
    "\n",
    "class Fuse_Loss(nn.Module):\n",
    "    def __init__(self, pixel_loss_func=Lovaz_Loss(), weight_image=0.05, weight_pixel=0.5, weight=1):\n",
    "        super(Fuse_Loss, self).__init__()\n",
    "        self.image_loss_func = F.cross_entropy\n",
    "        self.pixel_loss_func = pixel_loss_func\n",
    "        self.activation = torch.sigmoid\n",
    "        self.weight_image = weight_image\n",
    "        self.weight_pixel = weight_pixel\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, logits, logit_pixel, logit_image, truth_pixel, truth_image):\n",
    "        pixel_non_empty = logit_pixel[truth_image,:,:,:]\n",
    "        mask_non_empty = truth_pixel[truth_image,:,:,:]\n",
    "        #loss_pixel = (self.pixel_loss_func(pixel_non_empty, mask_non_empty)+self.pixel_loss_func(-pixel_non_empty, 1-mask_non_empty))/2\n",
    "        loss_pixel = self.pixel_loss_func(pixel_non_empty, mask_non_empty)\n",
    "        truth_image = truth_image.type(torch.LongTensor).to(device)\n",
    "        loss_image = self.image_loss_func(logit_image, truth_image, reduction='elementwise_mean')\n",
    "        #loss = (self.pixel_loss_func(logits, truth_pixel)+self.pixel_loss_func(-logits, 1-truth_pixel))/2\n",
    "        loss = self.pixel_loss_func(logits, truth_pixel)\n",
    "        return self.weight_pixel*loss_pixel + self.weight_image*loss_image + self.weight*loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f6879d5d3bbb6c70fd2c25908083b690e59c13a8"
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "\n",
    "ia.seed(2018)\n",
    "\n",
    "def _standardize(img):\n",
    "    return (img - img.map(np.mean)) / img.map(np.std)\n",
    "\n",
    "affine_seq = iaa.Sequential([\n",
    "    # General\n",
    "    iaa.SomeOf((1, 2),\n",
    "               [iaa.Fliplr(0.5),\n",
    "                iaa.Noop(),\n",
    "                ]),\n",
    "    iaa.Affine(rotate=(-5, 5), mode='reflect'),\n",
    "    iaa.Crop(px=(0, 10)),\n",
    "], random_order=True)\n",
    "\n",
    "intensity_seq = iaa.Sequential([\n",
    "    #iaa.Invert(0.3),\n",
    "    iaa.Sometimes(0.3, iaa.ContrastNormalization((0.5, 1.5))),\n",
    "    iaa.OneOf([\n",
    "        iaa.Noop(),\n",
    "        iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Add((-10, 10)),\n",
    "                iaa.AddElementwise((-10, 10)),\n",
    "                iaa.Multiply((0.95, 1.05)),\n",
    "                iaa.MultiplyElementwise((0.95, 1.05)),\n",
    "            ]),\n",
    "        ]),\n",
    "        iaa.OneOf([\n",
    "            iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "            iaa.AverageBlur(k=(2, 5)),\n",
    "            #iaa.MedianBlur(k=(3, 5))\n",
    "        ])\n",
    "    ])\n",
    "], random_order=False)\n",
    "\n",
    "tta_intensity_seq = iaa.Sequential([\n",
    "    iaa.Noop()\n",
    "], random_order=False)\n",
    "\n",
    "def compute_random_pad(limit=(-4,4)):\n",
    "    dy  = IMG_TAR_SIZE - IMG_ORI_SIZE*SCALE\n",
    "    dy0 = dy//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dy)\n",
    "    dy1 = dy - dy0\n",
    "    dx0 = dy//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dy)\n",
    "    dx1 = dy - dx0\n",
    "    return dy0, dx0, dy1, dx1\n",
    "\n",
    "def resize_pad_seq(pad_size):\n",
    "    dy0, dx0, dy1, dx1 = compute_random_pad()\n",
    "    seq = iaa.Sequential([\n",
    "        affine_seq,\n",
    "        iaa.Scale({'height': IMG_ORI_SIZE*SCALE, 'width': IMG_ORI_SIZE*SCALE}),\n",
    "        iaa.Pad(px=(dy0, dx0, dy1, dx1), pad_mode='edge', keep_size=False),\n",
    "    ], random_order=False)\n",
    "    return seq\n",
    "\n",
    "def resize_pad_seq_eval(pad_size):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Scale({'height': IMG_ORI_SIZE*SCALE, 'width': IMG_ORI_SIZE*SCALE}),\n",
    "        iaa.Pad(px=(pad_size, pad_size, pad_size+1, pad_size+1), pad_mode='edge', keep_size=False),\n",
    "    ], random_order=False)\n",
    "    return seq\n",
    "\n",
    "def resize_seq():\n",
    "    seq = iaa.Sequential([\n",
    "        affine_seq,\n",
    "        iaa.Scale({'height': IMG_TAR_SIZE, 'width': IMG_TAR_SIZE})\n",
    "    ], random_order=False)\n",
    "    return seq\n",
    "\n",
    "def resize_seq_eval():\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Scale({'height': IMG_TAR_SIZE, 'width': IMG_TAR_SIZE})\n",
    "    ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8cbfed0014f8791e5d308d3cb6107c45866bef6b"
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.backends.cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "## convert salt coverage to class\n",
    "def cov_to_class_1(mask):\n",
    "    border = 10\n",
    "    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n",
    "    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n",
    "\n",
    "    cover = (mask>0.5).sum()\n",
    "    if cover < 8:\n",
    "        return 0 # empty\n",
    "    if cover == ((mask*outer) > 0.5).sum():\n",
    "        return 1 #border\n",
    "    if np.all(mask==mask[0]):\n",
    "        return 2 #vertical\n",
    "\n",
    "    percentage = cover/(101*101)\n",
    "    if percentage < 0.15:\n",
    "        return 3\n",
    "    elif percentage < 0.25:\n",
    "        return 4\n",
    "    elif percentage < 0.50:\n",
    "        return 5\n",
    "    elif percentage < 0.75:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "    \n",
    "def cov_to_class_2(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "\n",
    "## used to load data from data files\n",
    "class my_DataLoader:\n",
    "    def __init__(self, train=False, test=False, Kfold=False, test_size=0.2):\n",
    "        self.test = test\n",
    "        self.train = train\n",
    "        self.Kfold = Kfold\n",
    "        self.test_size = test_size\n",
    "        self.num_fold = int(1/test_size)\n",
    "            \n",
    "        train_df, self.test_df = self._load_depth()\n",
    "        \n",
    "        if self.train:\n",
    "            self._load_image_mask(train_df)\n",
    "            train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(IMG_ORI_SIZE, 2)\n",
    "            train_df[\"coverage_class\"] = train_df.masks.map(cov_to_class_1)\n",
    "            self.x_train, self.x_valid, self.y_train, self.y_valid = self._get_train_test_split(train_df, self.Kfold, self.num_fold)\n",
    "            \n",
    "        if self.test:\n",
    "            self.test_df['images'] = self._load_image_test(self.test_df)\n",
    "            self.x_test = np.array(self.test_df.images.tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_mask(train_df):\n",
    "        # load image data & mask data\n",
    "        train_df['images'] = [np.array(cv2.imread(TRAIN_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n",
    "        train_df['masks'] = [np.array(cv2.imread(TRAIN_MASK_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n",
    "        # Normalize image vectors\n",
    "        #train_df['images'] /= 255\n",
    "        #train_df['masks'] /= 255\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_image_test(test_df):\n",
    "        return [np.array(cv2.imread(TEST_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(test_df.index)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_depth():\n",
    "        train_df = pd.read_csv(TRAIN_INFO_PATH, index_col=\"id\", usecols=[0])\n",
    "        depths_df = pd.read_csv(DEPTH_PATH, index_col=\"id\")\n",
    "        depths_df['z'] = depths_df['z'].astype('float')\n",
    "        train_df = train_df.join(depths_df)\n",
    "        test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "        return train_df, test_df\n",
    "\n",
    "    ## get train & validation split stratified by salt coverage\n",
    "    @staticmethod\n",
    "    def _get_train_test_split(train_df, Kfold, num_fold):\n",
    "        x_train, x_valid, y_train, y_valid = [], [], [], []\n",
    "        skf = StratifiedKFold(n_splits=num_fold, random_state=1234, shuffle=True)\n",
    "        for train_index, valid_index in skf.split(train_df.index.values, train_df.coverage_class):\n",
    "            x_tr = np.array(train_df.images[train_index].tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1)\n",
    "            x_tr = np.append(x_tr, [np.fliplr(x) for x in x_tr], axis=0)\n",
    "            x_train.append(x_tr)\n",
    "            x_valid.append(np.array(train_df.images[valid_index].tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1))\n",
    "            y_tr = np.array(train_df.masks[train_index].tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1)\n",
    "            y_tr = np.append(y_tr, [np.fliplr(y) for y in y_tr], axis=0)\n",
    "            y_train.append(y_tr)\n",
    "            y_valid.append(np.array(train_df.masks[valid_index].tolist()).reshape(-1, IMG_ORI_SIZE, IMG_ORI_SIZE, 1))\n",
    "            if not Kfold:\n",
    "                break\n",
    "        return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    def get_train(self):\n",
    "        return self.x_train, self.y_train\n",
    "    \n",
    "    def get_valid(self):\n",
    "        return self.x_valid, self.y_valid\n",
    "\n",
    "    def get_test_x(self):\n",
    "        return self.x_test\n",
    "\n",
    "    def get_test_df(self):\n",
    "        return self.test_df\n",
    "    \n",
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, mode='train', fit_method='resize'):\n",
    "        if mode == 'train' or mode == 'valid':\n",
    "            self.x = data[0]\n",
    "            self.y = data[1]\n",
    "        elif mode == 'test':\n",
    "            self.data = data\n",
    "        else:\n",
    "            raise RuntimeError('MODE_ERROR')\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.pad_method = PAD_METHOD\n",
    "        self.pad_size = (IMG_TAR_SIZE-IMG_ORI_SIZE)//2\n",
    "        \n",
    "        if fit_method == 'resize_pad':\n",
    "            self.aug_func_eval = partial(resize_pad_seq_eval, self.pad_size)\n",
    "        elif fit_method == 'resize':\n",
    "            self.aug_func_eval = resize_seq_eval\n",
    "            \n",
    "        if AUG:\n",
    "            if fit_method == 'resize_pad':\n",
    "                self.aug_func = partial(resize_pad_seq, self.pad_size)\n",
    "            elif fit_method == 'resize':\n",
    "                self.aug_func = resize_seq\n",
    "        \n",
    "        if INPUT_CHANNEL == 3:\n",
    "            self.depth = np.tile(np.linspace(0,1,IMG_TAR_SIZE),[IMG_TAR_SIZE,1]).T\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train' or self.mode == 'valid':\n",
    "            return len(self.x)\n",
    "        elif self.mode == 'test':\n",
    "            return len(self.data)\n",
    "        else:\n",
    "            raise RuntimeError('MODE_ERROR')\n",
    "               \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            if AUG:\n",
    "                resize_seq_det = self.aug_func().to_deterministic()\n",
    "                new_x_batch = resize_seq_det.augment_image(self.x[idx])\n",
    "                new_x_batch = intensity_seq.augment_image(new_x_batch)/255\n",
    "                new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n",
    "            else:\n",
    "                resize_seq_det = self.aug_func_eval().to_deterministic()\n",
    "                new_x_batch = resize_seq_det.augment_image(self.x[idx])/255\n",
    "                new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n",
    "            if INPUT_CHANNEL == 3:\n",
    "                new_x_batch = np.tile(new_x_batch,(1,1,3))\n",
    "                new_x_batch = add_depth_channels(new_x_batch, self.depth)\n",
    "            return new_x_batch, new_y_batch\n",
    "        elif self.mode == 'valid':\n",
    "            resize_seq_det = self.aug_func_eval().to_deterministic()\n",
    "            new_x_batch = resize_seq_det.augment_image(self.x[idx])/255\n",
    "            new_y_batch = resize_seq_det.augment_image(self.y[idx])/255\n",
    "            if INPUT_CHANNEL == 3:\n",
    "                new_x_batch = np.tile(new_x_batch,(1,1,3))\n",
    "                new_x_batch = add_depth_channels(new_x_batch, self.depth)\n",
    "            return new_x_batch, new_y_batch\n",
    "        elif self.mode == 'test':\n",
    "            resize_seq_det = self.aug_func_eval()\n",
    "            test_data = resize_seq_det.augment_image(self.data[idx])/255\n",
    "            if INPUT_CHANNEL == 3:\n",
    "                test_data = np.tile(test_data,(1,1,3))\n",
    "                new_x_batch = add_depth_channels(test_data, self.depth)\n",
    "            return test_data\n",
    "        else:\n",
    "            raise RuntimeError('MODE_ERROR')\n",
    "            \n",
    "def make_loader(data, batch_size, num_workers=4, shuffle=False, transform=None, mode='train', fit_method='resize'):\n",
    "        return DataLoader(\n",
    "            dataset=ShipDataset(data, transform=transform, mode=mode, fit_method=fit_method),\n",
    "            shuffle=shuffle,\n",
    "            num_workers = num_workers,\n",
    "            batch_size = batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "47a024a662f85e6cbe936b1bce96d1c471533ddc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a78f2c0a504d7f93463c6e17b6766a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1adad28b3184352a23a35601bd7f306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15764d7a1934f93b87197f905a736a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dl = my_DataLoader(test=True, Kfold=KFOLD)\n",
    "x_test = dl.get_test_x()\n",
    "test_df = dl.get_test_df()\n",
    "dl = my_DataLoader(train=True, Kfold=KFOLD)\n",
    "x_valid, y_valid = dl.get_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "dc7cb553fc8640deee4f67f53d19128da1e5737b"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.models.resnet\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\"\"\"\n",
    "This script has been taken (and modified) from :\n",
    "https://github.com/ternaus/TernausNet\n",
    "@ARTICLE{arXiv:1801.05746,\n",
    "         author = {V. Iglovikov and A. Shvets},\n",
    "          title = {TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation},\n",
    "        journal = {ArXiv e-prints},\n",
    "         eprint = {1801.05746}, \n",
    "           year = 2018\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "class ConvBn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, groups=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, groups=groups),\n",
    "                                  nn.BatchNorm2d(out_channels),\n",
    "                                  )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class NoOperation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "class CSE(nn.Module):\n",
    "    def __init__(self, in_ch, r=2):\n",
    "        super(CSE, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear_1 = nn.Linear(in_ch, in_ch//r)\n",
    "        self.linear_2 = nn.Linear(in_ch//r, in_ch)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channel_num, _, _ = x.size()\n",
    "        input_x = x\n",
    "\n",
    "        x = self.avg_pool(x).view(batch_size, channel_num)\n",
    "        x = F.relu(self.linear_1(x), inplace=True)\n",
    "        x = self.linear_2(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        x = torch.mul(x, input_x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSE(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(SSE, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, 1, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        x = torch.mul(x, input_x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SCSE(nn.Module):\n",
    "    def __init__(self, in_ch, r=2):\n",
    "        super(SCSE, self).__init__()\n",
    "\n",
    "        self.cSE = CSE(in_ch, r)\n",
    "        self.sSE = SSE(in_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cSE = self.cSE(x)\n",
    "        sSE = self.sSE(x)\n",
    "\n",
    "        x = torch.add(cSE,sSE)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, groups=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = ConvBn2d(in_channels, middle_channels, kernel_size=3, padding=1, groups=groups)\n",
    "        self.conv2 = ConvBn2d(middle_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.SCSE = SCSE(out_channels)\n",
    "        \n",
    "    def forward(self, x, e=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        if e is not None:\n",
    "            x = torch.cat([x,e], 1)\n",
    "            x = F.dropout2d(x, p = 0.50)\n",
    "            \n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = self.SCSE(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetResNet34_DS(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_2d=0.2, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        self.resnet = torchvision.models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            self.resnet.layer1,\n",
    "        )\n",
    "        self.encoder3 = self.resnet.layer2\n",
    "        self.encoder4 = self.resnet.layer3\n",
    "        self.encoder5 = self.resnet.layer4\n",
    "        \n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "        self.decoder5 = Decoder(256+512, 512, 64)\n",
    "        self.decoder4 = Decoder( 64+256, 256, 64)\n",
    "        self.decoder3 = Decoder( 64+128, 128, 64)\n",
    "        self.decoder2 = Decoder( 64+ 64,  64, 64)\n",
    "        self.decoder1 = Decoder( 64+ 64,  32, 64)\n",
    "        \n",
    "        self.fuse_pixel = nn.Sequential(\n",
    "            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.logit_pixel  = nn.Sequential(\n",
    "            nn.Conv2d( 64, 1, kernel_size=1, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.fuse_image = nn.Sequential(\n",
    "            nn.Conv2d(512, 64, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.logit_image = nn.Linear(64, 2)\n",
    "        \n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(64+64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.shape\n",
    "        mean= [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x-mean[2])/std[2],\n",
    "            (x-mean[1])/std[1],\n",
    "            (x-mean[0])/std[0],\n",
    "        ],1)\n",
    "        \"\"\"\n",
    "        if INPUT_CHANNEL == 1:\n",
    "            x = torch.cat([x,x,x],1)\n",
    "        \"\"\"\n",
    "        \n",
    "        e1 = self.encoder1(x) #;print('e1', e1.size())\n",
    "        e2 = self.encoder2(e1)#;print('e2', e2.size())\n",
    "        e3 = self.encoder3(e2)#;print('e3', e3.size())\n",
    "        e4 = self.encoder4(e3)#;print('e4', e4.size())\n",
    "        e5 = self.encoder5(e4)#;print('e5', e5.size())\n",
    "        \n",
    "        f = self.center(e5)        #;print('f', f.size())\n",
    "        \n",
    "        d5 = self.decoder5(f,e5)   #;print('d5', d5.size())\n",
    "        d4 = self.decoder4(d5,e4)  #;print('d4', d4.size())\n",
    "        d3 = self.decoder3(d4,e3)  #;print('d3', d3.size())\n",
    "        d2 = self.decoder2(d3,e2)  #;print('d2', d2.size())\n",
    "        d1 = self.decoder1(d2,e1)  #;print('d1', d1.size())\n",
    "        \n",
    "        #hyper column\n",
    "        d = torch.cat((\n",
    "            d1,\n",
    "            F.interpolate(d2, scale_factor= 2, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d3, scale_factor= 4, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d4, scale_factor= 8, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d5, scale_factor=16, mode='bilinear', align_corners=False),\n",
    "        ), 1)\n",
    "        d = F.dropout2d(d, p=self.dropout_2d)\n",
    "        fuse_pixel = self.fuse_pixel(d)\n",
    "        logit_pixel = self.logit_pixel(fuse_pixel)\n",
    "        \n",
    "        e = F.adaptive_avg_pool2d(e5, output_size=[1,1])\n",
    "        e = F.dropout(e, p=self.dropout_2d)\n",
    "        fuse_image = self.fuse_image(e)\n",
    "        fuse_image_flatten = fuse_image.view(fuse_image.size(0), -1)\n",
    "        logit_image = self.logit_image(fuse_image_flatten)\n",
    "        \n",
    "        logit = self.logit(torch.cat([\n",
    "            fuse_pixel, \n",
    "            F.interpolate(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest')],1))\n",
    "\n",
    "        return logit, logit_pixel, logit_image\n",
    "    \n",
    "\n",
    "class UNetResNext50_DS(nn.Module):\n",
    "    def __init__(self, dropout_2d=0.2, pretrained='imagenet'):\n",
    "        super(UNetResNext50_DS, self).__init__()\n",
    "        self.dropout_2d = dropout_2d\n",
    "        self.pretrained = pretrained\n",
    "        self.encoder = se_resnext50_32x4d(pretrained=self.pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.encoder1 = nn.Sequential(self.encoder.layer0.conv1,\n",
    "                                   self.encoder.layer0.bn1,\n",
    "                                   self.encoder.layer0.relu1)\n",
    "\n",
    "        self.encoder2 = self.encoder.layer1\n",
    "        self.encoder3 = self.encoder.layer2\n",
    "        self.encoder4 = self.encoder.layer3\n",
    "        self.encoder5 = self.encoder.layer4\n",
    "\n",
    "        self.center = nn.Sequential(nn.Conv2d(512*4, 512, kernel_size=3,padding=1, groups=4),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "\n",
    "        self.decoder5 = Decoder(256 + 512*4, 512, 64)\n",
    "        self.decoder4 = Decoder(64 + 256*4, 256, 64)\n",
    "        self.decoder3 = Decoder(64 + 128*4, 128, 64)\n",
    "        self.decoder2 = Decoder(64 + 64*4, 64, 64)\n",
    "        self.decoder1 = Decoder(64, 32, 64)\n",
    "\n",
    "        self.fuse_pixel = nn.Sequential(\n",
    "            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.logit_pixel  = nn.Sequential(\n",
    "            nn.Conv2d( 64, 1, kernel_size=1, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.fuse_image = nn.Sequential(\n",
    "            nn.Conv2d(512*4, 64, kernel_size=1, groups=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.logit_image = nn.Linear(64, 2)\n",
    "        \n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(64+64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.shape\n",
    "        '''\n",
    "        mean= [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x-mean[2])/std[2],\n",
    "            (x-mean[1])/std[1],\n",
    "            (x-mean[0])/std[0],\n",
    "        ],1)\n",
    "        '''\n",
    "        if INPUT_CHANNEL == 1:\n",
    "            x = torch.cat([x,x,x],1)\n",
    "        \n",
    "        e1 = self.encoder1(x) #;print('e1', e1.size())\n",
    "        e2 = self.encoder2(e1)#;print('e2', e2.size())\n",
    "        e3 = self.encoder3(e2)#;print('e3', e3.size())\n",
    "        e4 = self.encoder4(e3)#;print('e4', e4.size())\n",
    "        e5 = self.encoder5(e4)#;print('e5', e5.size())\n",
    "        \n",
    "        f = self.center(e5)        #;print('f', f.size())\n",
    "        \n",
    "        d5 = self.decoder5(f,e5)   #;print('d5', d5.size())\n",
    "        d4 = self.decoder4(d5,e4)  #;print('d4', d4.size())\n",
    "        d3 = self.decoder3(d4,e3)  #;print('d3', d3.size())\n",
    "        d2 = self.decoder2(d3,e2)  #;print('d2', d2.size())\n",
    "        d1 = self.decoder1(d2)  #;print('d1', d1.size())\n",
    "        \n",
    "        #hyper column\n",
    "        d = torch.cat((\n",
    "            d1,\n",
    "            F.interpolate(d2, scale_factor= 2, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d3, scale_factor= 4, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d4, scale_factor= 8, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d5, scale_factor=16, mode='bilinear', align_corners=False),\n",
    "        ), 1)\n",
    "        d = F.dropout2d(d, p=self.dropout_2d)\n",
    "        fuse_pixel = self.fuse_pixel(d)\n",
    "        logit_pixel = self.logit_pixel(fuse_pixel)\n",
    "        \n",
    "        e = F.adaptive_avg_pool2d(e5, output_size=[1,1])\n",
    "        e = F.dropout(e, p=self.dropout_2d)\n",
    "        fuse_image = self.fuse_image(e)\n",
    "        fuse_image_flatten = fuse_image.view(fuse_image.size(0), -1)\n",
    "        logit_image = self.logit_image(fuse_image_flatten)\n",
    "        \n",
    "        logit = self.logit(torch.cat([\n",
    "            fuse_pixel, \n",
    "            F.interpolate(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest')],1))\n",
    "\n",
    "        return logit, logit_pixel, logit_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "3cb9d9f21ff3fb931606e04c7999b0921b20dc38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'model_best_resnext50_lovasz_1_0_0_iou_resize_pad.pth.tar'\n",
      "=> loading checkpoint 'model_best_resnext50_lovasz_1_0_0_iou_resize.pth.tar'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd1d4542af24008a9c8a3fc0be0a476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_pad = UNetResNext50_DS(dropout_2d=0.5, pretrained=None).to(device)\n",
    "model_resize = UNetResNext50_DS(dropout_2d=0.5, pretrained=None).to(device)\n",
    "\n",
    "if LOAD_CHECKPONT:\n",
    "        resume_path_pad = 'model_best_resnext50_lovasz_1_0_0_iou_resize_pad.pth.tar'\n",
    "        if os.path.isfile(resume_path_pad):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume_path_pad))\n",
    "            checkpoint_pad = torch.load(resume_path_pad)\n",
    "            #start_epoch = checkpoint['epoch']\n",
    "            #best_iou = checkpoint['best_iou']\n",
    "            model_pad.load_state_dict(checkpoint_pad['state_dict'])\n",
    "        resume_path_resize = 'model_best_resnext50_lovasz_1_0_0_iou_resize.pth.tar'\n",
    "        if os.path.isfile(resume_path_resize):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume_path_resize))\n",
    "            checkpoint_resize = torch.load(resume_path_resize)\n",
    "            #start_epoch = checkpoint['epoch']\n",
    "            #best_iou = checkpoint['best_iou']\n",
    "            model_resize.load_state_dict(checkpoint_resize['state_dict'])\n",
    "model_pad.eval()\n",
    "model_resize.eval()\n",
    "output_pad_list = []\n",
    "output_resize_list = []\n",
    "target_pad_list = []\n",
    "target_resize_list = []\n",
    "valid_loader_pad = make_loader((x_valid[0], y_valid[0]), num_workers=0, batch_size=64, mode='valid', fit_method='resize_pad')\n",
    "valid_loader_resize = make_loader((x_valid[0], y_valid[0]), num_workers=0, batch_size=64, mode='valid', fit_method='resize')\n",
    "with torch.no_grad():\n",
    "    for i, ((inputs_pad,targets_pad),(inputs_resize,targets_resize)) in tqdm_notebook(enumerate(zip(valid_loader_pad, valid_loader_resize))):\n",
    "        inputs_pad_1 = inputs_pad.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        targets_pad = targets_pad.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        inputs_resize_1 = inputs_resize.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        targets_resize = targets_resize.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        inputs_pad_2 = inputs_pad_1.flip(3)\n",
    "        inputs_resize_2 = inputs_resize_1.flip(3)\n",
    "        outputs_pad_1, _, _ = model_pad(inputs_pad_1)\n",
    "        outputs_pad_2, _, _ = model_pad(inputs_pad_2)\n",
    "        output_pad_list += [(outputs_pad_1+outputs_pad_2.flip(3))/2]\n",
    "        outputs_resize_1, _, _ = model_resize(inputs_resize_1)\n",
    "        outputs_resize_2, _, _ = model_resize(inputs_resize_2)\n",
    "        output_resize_list += [(outputs_resize_1+outputs_resize_2.flip(3))/2]\n",
    "        target_pad_list += [targets_pad]\n",
    "        target_resize_list += [targets_resize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f51ba0359739855d192602bd2439babca1229e2c"
   },
   "outputs": [],
   "source": [
    "outputs_pad = torch.cat(output_pad_list,0)\n",
    "outputs_resize = torch.cat(output_resize_list,0)\n",
    "targets_pad = torch.cat(target_pad_list,0)\n",
    "targets_resize = torch.cat(target_resize_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "ac749282a410773f981143acc039a2c2c3f72875"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161d395101bc4f5d9a0cef0c03be9c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8f65591b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thrs = np.linspace(0.3, 0.7, 31)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "thrs = np.log(thrs/(1-thrs)) \n",
    "ious = np.array([my_iou_metric_fuse_thrs(get_numpy(targets_pad), get_numpy(outputs_pad), get_numpy(outputs_resize), threshold) for threshold in tqdm_notebook(thrs)])\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thrs[threshold_best_index]\n",
    "\n",
    "plt.plot(thrs, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "605ab27f3a682cc436fb3d5e328733960e86eeac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435873733dd3405f80862b33fcbe9d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader_resize = make_loader(x_test, num_workers=0, batch_size=64, mode='test', fit_method='resize')\n",
    "test_loader_pad = make_loader(x_test, num_workers=0, batch_size=64, mode='test', fit_method='resize_pad')\n",
    "model_resize.eval()\n",
    "model_pad.eval()\n",
    "output_resize_list = []\n",
    "output_pad_list = []\n",
    "with torch.no_grad():\n",
    "    for inputs_resize, inputs_pad in tqdm_notebook(zip(test_loader_resize, test_loader_pad)):\n",
    "        inputs_resize_1 = inputs_resize.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        inputs_resize_2 = inputs_resize_1.flip(3)\n",
    "        inputs_pad_1 = inputs_pad.permute(0,3,1,2).type(torch.FloatTensor).to(device)\n",
    "        inputs_pad_2 = inputs_pad_1.flip(3)\n",
    "        outputs_resize_1, _, _ = model_resize(inputs_resize_1)\n",
    "        outputs_resize_2, _, _ = model_resize(inputs_resize_2)\n",
    "        outputs_pad_1, _, _ = model_pad(inputs_pad_1)\n",
    "        outputs_pad_2, _, _ = model_pad(inputs_pad_2)\n",
    "        output_resize_list += [(outputs_resize_1+outputs_resize_2.flip(3))/2]\n",
    "        output_pad_list += [(outputs_pad_1+outputs_pad_2.flip(3))/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3d5ed5c878543c9cf53be150e0a4f8a8cd20f720"
   },
   "outputs": [],
   "source": [
    "outputs_pad = torch.cat(output_pad_list,0)\n",
    "outputs_resize = torch.cat(output_resize_list,0)\n",
    "outputs_pad_array = list(get_numpy(outputs_pad.squeeze(1)))\n",
    "outputs_resize_array = list(get_numpy(outputs_resize.squeeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "4cd2ed084226ba1ac06d4e4c14ae9a8c6eab6a6b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "used for converting the decoded image to rle mask\n",
    "Fast compared to previous one\n",
    "\"\"\"\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "949a2f027f9545c3abd6f403aa5152509d4cd7d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc5a3420c043b7b7bd05140591de7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = {idx: rle_encode(recover_size_pad_resize(outputs_pad_array[i], outputs_resize_array[i], threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "e6db737c4f114e151bbabdbdb966bde4ca0ffac3"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "submission_file = 'submission_{name}_{loss}_{mode}_{aug}.cvs'.format(name='resnext50',\n",
    "                                                                 loss='bce_dice',\n",
    "                                                                 mode='iou',\n",
    "                                                                 aug='resize_pad_pad')\n",
    "sub.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
