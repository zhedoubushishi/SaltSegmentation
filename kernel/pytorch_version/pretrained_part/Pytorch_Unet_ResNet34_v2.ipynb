{
  "cells": [
    {
      "metadata": {
        "_uuid": "54643e32b4132d15df20d55f2b9ee09efc2c51a8"
      },
      "cell_type": "markdown",
      "source": "# Libraries"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b15761a022f76bb50db1bb55c21b59a3f1c88261"
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy import ndimage\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage import filters\n\nfrom imgaug import augmenters as iaa\n\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torchvision.transforms import ToTensor, ToPILImage, Normalize, Compose\n\nimport PIL\n\nfrom datetime import datetime\nimport json\nimport gc\n\nimport time\nt_start = time.time()",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "99e7207d6aec7d921e4b8ad4f30d310d2346216a"
      },
      "cell_type": "markdown",
      "source": "# Global variable"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73cc76d8506e96b912600406fa5ceeb47083978a"
      },
      "cell_type": "code",
      "source": "TRAIN_IMG_PATH = \"../input/train/images/\"\nTEST_IMG_PATH = \"../input/test/images/\"\nDEPTH_PATH = \"../input/depths.csv\"\nTRAIN_MASK_PATH = \"../input/train/masks/\"\nTRAIN_INFO_PATH = \"../input/train.csv\"\n\n# basic parameters\nIMG_ORI_SIZE = 101\nIMG_TAR_SIZE = 101\n\n# Keras Model parameters\nSTART_NEURONS = 16\nDROPOUT_RATIO = 0.5\n\nMODEL1_ADAM_LR = 0.01\nMODEL1_EPOCHS = 100\nMODEL1_BATCH_SIZE = 64\nMODEL1_STEPS_PER_EPOCH_TRAIN = 200\nMODEL1_LOSS = 'binary_crossentropy'\n\nMODEL2_ADAM_LR = 0.01\nMODEL2_EPOCHS = 100\nMODEL2_BATCH_SIZE = 64\nMODEL2_STEPS_PER_EPOCH_TRAIN = 200\nMODEL2_LOSS = 'lovasz_loss'\n\n# ReduceLROnPlateau parameters\nMODEL1_REDUCE_FACTOR = 0.5\nMODEL1_REDUCE_PATIENT = 5\n\nMODEL2_REDUCE_FACTOR = 0.5\nMODEL2_REDUCE_PATIENT = 5\n\nversion = 1\n\n# Name\nBASIC_NAME = f'Unet_resnet_v{version}'\nSAVE_MODEL_NAME = BASIC_NAME + '.model'\nSUBMISSION_NAME = BASIC_NAME + '.csv'",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa113e7f4cabb11b9a14afc2151e96ff664ef7ed"
      },
      "cell_type": "code",
      "source": "# encoding=utf8\nimport cv2\nimport numpy as np\nfrom torch import nn\n\n################################################################################\n# related functions & loss functions\n################################################################################\n\n\ndef upsample(img):\n    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n        return img\n    return cv2.resize(img, (IMG_TAR_SIZE, IMG_TAR_SIZE))\n\n\ndef downsample(img):\n    if IMG_ORI_SIZE == IMG_TAR_SIZE:\n        return img\n    return cv2.resize(img, (IMG_ORI_SIZE, IMG_ORI_SIZE))\n\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\ndef write_event(log, step: int, **data):\n    data['step'] = step\n    data['dt'] = datetime.now().isoformat()\n    log.write(json.dumps(data, sort_keys=True, cls=MyEncoder))\n    log.write('\\n')\n    log.flush()\n\ndef get_variable(x):\n    \"\"\" Converts tensors to cuda, if available. \"\"\"\n    if torch.cuda.is_available():\n        return x.cuda()\n    return x\n\ndef get_numpy(x):\n    \"\"\" Get numpy array for both cuda and not. \"\"\"\n    if torch.cuda.is_available():\n        return x.cpu().data.numpy()\n    return x.data.numpy()\n    \ndef iou_numpy(outputs, labels):\n    SMOOTH = 1e-6\n    labels = labels.squeeze(1)\n    outputs = outputs.squeeze(1)\n    \n    intersection = (outputs & labels).sum((1, 2))\n    union = (outputs | labels).sum((1, 2))\n    \n    iou = (intersection + SMOOTH) / (union + SMOOTH)\n    \n    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n    \n    return thresholded.mean()\n\n\ndef my_iou_metric(label, pred):\n    return iou_numpy(pred > 0.5, label>0.5)\n\n\ndef my_iou_metric_2(label, pred):\n    return iou_numpy(pred > 0, label>0.5)\n\n\nclass DICELoss(nn.Module):\n \n    def __init__(self):\n        super(DICELoss, self).__init__()\n \n    def forward(self, output, mask):\n \n        probs = torch.squeeze(output, 1)\n        mask = torch.squeeze(mask, 1)\n \n        intersection = probs * mask\n        intersection = torch.sum(intersection, 2)\n        intersection = torch.sum(intersection, 1)\n \n        # print( num )\n \n        den1 = probs * probs\n        # print(den1.size())\n        den1 = torch.sum(den1, 2)\n        den1 = torch.sum(den1, 1)\n \n        # print(den1.size())\n \n        den2 = mask * mask\n        # print(den2.size())\n        den2 = torch.sum(den2, 2)\n        den2 = torch.sum(den2, 1)\n \n        # print(den2.size())\n        eps = 0.0000001\n        dice = 2 * ((intersection + eps) / (den1 + den2 + eps))\n        # dice_eso = dice[:, 1:]\n        dice_eso = dice\n \n        loss = 1 - torch.sum(dice_eso) / dice_eso.size(0)\n        return loss\n    \n\nclass BCE_DICE_Loss(nn.Module):\n \n    def __init__(self):\n        super(BCE_DICE_Loss, self).__init__()\n \n    def forward(self, output, mask):\n        criterion_1 = nn.BCELoss()\n        criterion_2 = DICELoss()\n        loss = criterion_1(output, mask) + criterion_2(output, mask)\n        return loss   ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6879d5d3bbb6c70fd2c25908083b690e59c13a8"
      },
      "cell_type": "code",
      "source": "import imgaug as ia\nfrom imgaug import augmenters as iaa\nimport numpy as np\n\nia.seed(2018)\n\n\ndef _standardize(img):\n    return (img - img.map(np.mean)) / img.map(np.std)\n\n\nst = lambda aug: iaa.Sometimes(0.5, aug)\naffine_seq = iaa.Sequential([\n    # General\n    st(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)} # translate by -16 to +16 pixels (per axis)\n        )),\n    # Deformations\n    iaa.Sometimes(0.3, iaa.PiecewiseAffine(scale=(0.04, 0.08))),\n    iaa.Sometimes(0.3, iaa.PerspectiveTransform(scale=(0.05, 0.1))),\n], random_order=True)\n\nintensity_seq = iaa.Sequential([\n    iaa.Invert(0.3),\n    iaa.Sometimes(0.3, iaa.ContrastNormalization((0.5, 1.5))),\n    iaa.OneOf([\n        iaa.Noop(),\n        iaa.Sequential([\n            iaa.OneOf([\n                iaa.Add((-10, 10)),\n                iaa.AddElementwise((-10, 10)),\n                iaa.Multiply((0.95, 1.05)),\n                iaa.MultiplyElementwise((0.95, 1.05)),\n            ]),\n        ]),\n        iaa.OneOf([\n            iaa.GaussianBlur(sigma=(0.0, 1.0)),\n            iaa.AverageBlur(k=(2, 5)),\n            #iaa.MedianBlur(k=(3, 5))\n        ])\n    ])\n], random_order=False)\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8cbfed0014f8791e5d308d3cb6107c45866bef6b"
      },
      "cell_type": "code",
      "source": "# encoding=utf8\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\n## convert salt coverage to class\ndef _cov_to_class(val):\n    for i in range(0, 11):\n        if val * 10 <= i:\n            return i\n\n## used to load data from data files\nclass my_DataLoader:\n\n    def __init__(self, train=True, test=True, Kflod=False, test_size=0.2, num_flod=5):\n        self.test = test\n        self.train = train\n        self.Kflod = Kflod\n        self.test_size = test_size\n        \n        if self.Kflod:\n            self.num_flod = num_flod\n        else:\n            self.num_flod = 0\n        \n        if self.train:\n            train_df, self.test_df = self._load_depth()\n            self._load_image_mask(train_df, self.test_df)\n            train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(IMG_ORI_SIZE, 2)\n            train_df[\"coverage_class\"] = train_df.coverage.map(_cov_to_class)\n            self.x_train, self.x_valid, self.y_train, self.y_valid = self._get_train_test_split(train_df)\n            train_df = None\n            \n        if self.test:\n            self.x_test = np.array(self.test_df.images.tolist()).reshape(-1, IMG_TAR_SIZE, IMG_TAR_SIZE, 1)\n\n    @staticmethod\n    def _load_image_mask(train_df, test_df):\n        # load image data & mask data\n        train_df['images'] = [np.array(cv2.imread(TRAIN_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n        train_df['masks'] = [np.array(cv2.imread(TRAIN_MASK_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(train_df.index)]\n        test_df['images'] = [np.array(cv2.imread(TEST_IMG_PATH + \"{}.png\".format(idx), 0)) for idx in tqdm_notebook(test_df.index)]\n        # Normalize image vectors\n        train_df['images'] /= 255\n        test_df['images'] /= 255\n        train_df['masks'] /= 255\n\n    @staticmethod\n    def _load_depth():\n        train_df = pd.read_csv(TRAIN_INFO_PATH, index_col=\"id\", usecols=[0])\n        depths_df = pd.read_csv(DEPTH_PATH, index_col=\"id\")\n        depths_df['z'] = depths_df['z'].astype('float')\n        train_df = train_df.join(depths_df)\n        test_df = depths_df[~depths_df.index.isin(train_df.index)]\n        return train_df, test_df\n\n    ## get train & validation split stratified by salt coverage\n    @staticmethod\n    def _get_train_test_split(train_df):\n        x_train, x_valid, y_train, y_valid = train_test_split(\n            np.array(train_df.images.map(upsample).tolist()).reshape(-1, IMG_TAR_SIZE, IMG_TAR_SIZE, 1),\n            np.array(train_df.masks.map(upsample).tolist()).reshape(-1, IMG_TAR_SIZE, IMG_TAR_SIZE, 1),\n            test_size=0.2, stratify=train_df.coverage_class, random_state=1234)\n        return x_train, x_valid, y_train, y_valid\n\n    def get_train(self):\n        return self.x_train, self.y_train\n    \n    def get_valid(self):\n        return self.x_valid, self.y_valid\n\n    def get_test_x(self):\n        return self.x_test\n\n    def get_test_df(self):\n        return self.test_df\n    \nclass ShipDataset(Dataset):\n    def __init__(self, data, transform=None, mode='train'):\n        if mode == 'train' or mode == 'valid':\n            self.x = np.transpose(data[0], (0,3,1,2))\n            self.y = np.transpose(data[1], (0,3,1,2))\n        elif mode == 'test':\n            self.data = np.transpose(data, (0,3,1,2))\n        else:\n            raise RuntimeError('MODE_ERROR')\n        self.transform = transform\n        self.mode = mode\n\n    def __len__(self):\n        if self.mode == 'train' or self.mode == 'valid':\n            return len(self.x)\n        elif self.mode == 'test':\n            return len(self.data)\n        else:\n            raise RuntimeError('MODE_ERROR')\n               \n    def __getitem__(self, idx):\n        if self.mode == 'train' or self.mode == 'valid':\n            return torch.from_numpy(self.x[idx]), torch.from_numpy(self.y[idx])\n        elif self.mode == 'test':\n            return torch.from_numpy(self.data[idx])\n        else:\n            raise RuntimeError('MODE_ERROR')\n            \ndef make_loader(data, batch_size, num_workers=4, shuffle=False, transform=None, mode='train'):\n        return DataLoader(\n            dataset=ShipDataset(data, transform=transform, mode=mode),\n            shuffle=shuffle,\n            num_workers = num_workers,\n            batch_size = batch_size,\n            pin_memory=torch.cuda.is_available()\n        )",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47a024a662f85e6cbe936b1bce96d1c471533ddc"
      },
      "cell_type": "code",
      "source": "dl = my_DataLoader()\nx_train, y_train = dl.get_train()\nx_valid, y_valid = dl.get_valid()\n\n#Data augmentation\nx_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50444b51896548c18913060b51a345c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f393751f26cc40f8b66d94abc5c69cad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12b32b6b5dc14ee5acb57d218be776bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbc578c9bfb6b93b501b74b721a456c0a60ef0f0"
      },
      "cell_type": "code",
      "source": "train_loader = make_loader((x_train, y_train), batch_size=32, shuffle=True)\nvalid_loader = make_loader((x_valid, y_valid), num_workers=0, batch_size=64, mode='valid')",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "438c676e325d9eea868ee669225fe6bae0eeb769"
      },
      "cell_type": "code",
      "source": "# encoding=utf8\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n################################################################################\n# train u-net & resnet model with Keras\n################################################################################\nclass BatchActivate(nn.Module):\n    def __init__(self, out_ch):\n        super(BatchActivate, self).__init__()\n        self.BA = nn.Sequential(\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        x = self.BA(x)\n        return x\n\nclass convolution_block(nn.Module):\n    def __init__(self, in_ch, out_ch, size, strides=(1,1), padding=(1,1), activation=True):\n        super(convolution_block, self).__init__()\n        \n        if activation == True:\n            self.CB = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, size, stride=strides, padding=(1, 1)),\n                BatchActivate(out_ch)\n            )\n        else:\n            self.CB = nn.Conv2d(in_ch, out_ch, size, stride=strides, padding=(1, 1))\n            \n    def forward(self, x):\n        x = self.CB(x)\n        return x\n    \nclass residual_block(nn.Module):\n    def __init__(self, out_ch, batch_activation=False):\n        super(residual_block, self).__init__()\n        self.BAC = nn.Sequential(\n            BatchActivate(out_ch),\n            convolution_block(out_ch, out_ch, (3,3)),\n            convolution_block(out_ch, out_ch, (3,3), activation=False)\n        )\n        self.BA = BatchActivate(out_ch)\n        self.batch_activation = batch_activation\n        \n    def forward(self, blockinput):\n        x = self.BAC(blockinput)\n        x = torch.add(blockinput, 1, x)\n        \n        if self.batch_activation:\n            return self.BA(x)\n        else:\n            return x\n        \nclass up_block(nn.Module):\n    def __init__(self, in_ch, out_ch, DropoutRatio=0.5, padding=0, output_padding=0):\n        super(up_block, self).__init__()\n        self.UP = nn.ConvTranspose2d(in_ch, out_ch, (3,3), (2,2), padding, output_padding)\n        self.DP = nn.Dropout(DropoutRatio)\n            \n        self.CR = nn.Sequential(\n            nn.Conv2d(out_ch*2, out_ch, (3,3), padding=(1, 1)),\n            residual_block(out_ch),\n            residual_block(out_ch, True)\n        )\n        \n    def forward(self, xb, xa):\n        xa = self.UP(xa)\n        x = torch.cat([xa, xb], dim=1)\n        x = self.DP(x)\n        x = self.CR(x)\n        return x\n    \n\n        \nclass Res_UNet(nn.Module):\n    def __init__(self, start_neurons):\n        super(Res_UNet, self).__init__()\n             \n        self.d1 = nn.Sequential(\n            nn.Conv2d(1, start_neurons * 1, (3,3), padding=(1, 1)),\n            residual_block(start_neurons * 1),\n            residual_block(start_neurons * 1, True),\n        )\n        \n        self.d2 = nn.Sequential(\n            nn.Conv2d(start_neurons * 1, start_neurons * 2, (3,3), padding=(1, 1)),\n            residual_block(start_neurons * 2),\n            residual_block(start_neurons * 2, True),\n        )\n        \n        self.d3 = nn.Sequential(\n            nn.Conv2d(start_neurons * 2, start_neurons * 4, (3,3), padding=(1, 1)),\n            residual_block(start_neurons * 4),\n            residual_block(start_neurons * 4, True),\n        )\n        \n        self.d4 = nn.Sequential(\n            nn.Conv2d(start_neurons * 4, start_neurons * 8, (3,3), padding=(1, 1)),\n            residual_block(start_neurons * 8),\n            residual_block(start_neurons * 8, True),\n        )\n        \n        self.m = nn.Sequential(\n            nn.Conv2d(start_neurons * 8, start_neurons * 16, (3,3), padding=(1, 1)),\n            residual_block(start_neurons * 16),\n            residual_block(start_neurons * 16, True)\n        )\n        \n        self.up4 = up_block(start_neurons * 16, start_neurons * 8, padding=1, output_padding=1)\n        \n        self.up5 = up_block(start_neurons * 8, start_neurons * 4)\n        \n        self.up6 = up_block(start_neurons * 4, start_neurons * 2, padding=1, output_padding=1)\n        \n        self.up7 = up_block(start_neurons * 2, start_neurons * 1)\n        \n        self.conv2d_noactiv = nn.Conv2d(start_neurons * 1, 1, (1,1))\n        \n        self.activ = nn.Softmax()\n        \n    def forward(self, x, DropOutRatio=0.5):\n        x1 = self.d1(x)\n        x1d = F.dropout(F.max_pool2d(x1, 2), DropOutRatio/2)\n        x2 = self.d2(x1d)\n        x2d = F.dropout(F.max_pool2d(x2, 2), DropOutRatio)\n        x3 = self.d3(x2d)\n        x3d = F.dropout(F.max_pool2d(x3, 2), DropOutRatio)\n        x4 = self.d4(x3d)\n        x4d = F.dropout(F.max_pool2d(x4, 2), DropOutRatio)\n        x5 = self.m(x4d)\n        x6 = self.up4(x4, x5)\n        x7 = self.up5(x3, x6)\n        x8 = self.up6(x2, x7)\n        x9 = self.up7(x1, x8)\n        x_noactiv = self.conv2d_noactiv(x9)\n        output = torch.sigmoid(x_noactiv)\n        return output",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1e5972b534849e0cec3405de9ba678904411afe"
      },
      "cell_type": "code",
      "source": "import torch.optim as optim\nfrom torch.autograd import Variable\n\nres_unet = Res_UNet(16).double()\ncriterion = BCE_DICE_Loss()",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66be681ece2c0b85c8c0db265979c57cd7097569"
      },
      "cell_type": "code",
      "source": "def validation(model: nn.Module, criterion, valid_loader):\n    model.eval()\n    losses = []\n    iou = []\n    for inputs, targets in valid_loader:\n        inputs = torch.DoubleTensor(inputs).cuda()\n        targets = torch.DoubleTensor(targets).cuda()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        losses += [loss.item()]\n        iou += [my_iou_metric(get_numpy(targets), get_numpy(outputs))]\n\n    valid_loss = np.mean(losses)  # type: float\n\n    valid_iou = np.mean(iou)\n    \n    metrics = {'val_loss': valid_loss, 'val_iou': valid_iou}\n    return metrics\n\ndef train(lr, model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=None, fold=None):\n    optimizer = init_optimizer(lr)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', verbose=True, patience=3, factor=0.5, min_lr=0.0001)\n    #model = nn.DataParallel(model, device_ids=None)\n    \n    if torch.cuda.is_available():\n        model = model.cuda()\n        criterion = criterion.cuda()\n    \n    epoch = 1\n    step = 0\n    \n    report_each = 10\n\n    valid_losses = []\n    valid_ious = []\n    \n    valid_iou = 0\n    valid_loss = 0\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        scheduler.step(valid_iou)\n       \n        losses = []\n        ious = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            mean_iou = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs = torch.DoubleTensor(inputs).cuda()\n                targets = torch.DoubleTensor(targets).cuda()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                losses += [loss.item()]\n                ious += [my_iou_metric(get_numpy(targets), get_numpy(outputs))]\n                mean_loss = np.mean(losses[-report_each:])\n                mean_iou = np.mean(ious[-report_each:])\n                \n                if i % report_each == 0:\n                    print('Epoch: [{0}][{1}/{2}]\\t'\n                          'Loss {loss:.4f} ({loss_avg:.4f})\\t'\n                          'IOU {iou:.3f} ({iou_avg:.3f})'.format(\n                           epoch, i, len(tl), loss=losses[-1], loss_avg=mean_loss, iou=ious[-1], iou_avg=mean_iou))\n            valid_metrics = validation(model, criterion, valid_loader)\n            valid_loss = valid_metrics['val_loss']\n            valid_iou = valid_metrics['val_iou']\n            valid_losses += [valid_loss]\n            valid_ious += [valid_iou]\n            print('Epoch: [{0}][Validation]\\t' \n                  'Val_Loss: {val_loss:.5f}\\t' \n                  'Val_IOU: {val_iou:.5f}'.format(epoch, val_loss=valid_loss, val_iou=valid_iou))\n        except KeyboardInterrupt:\n            print('Ctrl+C, saving snapshot')\n            print('done.')\n            return  ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "612fb08d11220c8c50f8188519d6fe80c07fc4bc"
      },
      "cell_type": "code",
      "source": "train(init_optimizer=lambda lr: Adam(res_unet.parameters(), lr=lr),\n        lr = 0.01,\n        n_epochs = 50,\n        model=res_unet,\n        criterion=criterion,\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n        validation=validation,\n        fold=1)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: [1][0/200]\tLoss 0.8979 (0.8979)\tIOU 0.306 (0.306)\nEpoch: [1][10/200]\tLoss 0.9794 (1.1554)\tIOU 0.356 (0.284)\nEpoch: [1][20/200]\tLoss 1.0207 (1.0559)\tIOU 0.362 (0.393)\nEpoch: [1][30/200]\tLoss 1.1084 (1.0781)\tIOU 0.194 (0.307)\nEpoch: [1][40/200]\tLoss 1.0910 (0.9836)\tIOU 0.328 (0.339)\nEpoch: [1][50/200]\tLoss 0.7939 (0.9639)\tIOU 0.522 (0.428)\nEpoch: [1][60/200]\tLoss 0.8139 (1.0329)\tIOU 0.525 (0.497)\nEpoch: [1][70/200]\tLoss 1.0127 (0.9801)\tIOU 0.334 (0.456)\nEpoch: [1][80/200]\tLoss 0.8467 (0.9923)\tIOU 0.428 (0.331)\nEpoch: [1][90/200]\tLoss 0.9110 (0.9172)\tIOU 0.447 (0.490)\nEpoch: [1][100/200]\tLoss 0.8812 (0.9473)\tIOU 0.572 (0.470)\nEpoch: [1][110/200]\tLoss 1.0877 (0.9629)\tIOU 0.400 (0.472)\nEpoch: [1][120/200]\tLoss 0.8397 (0.9576)\tIOU 0.516 (0.464)\nEpoch: [1][130/200]\tLoss 0.9596 (0.9749)\tIOU 0.613 (0.544)\nEpoch: [1][140/200]\tLoss 0.9574 (0.9363)\tIOU 0.319 (0.402)\nEpoch: [1][150/200]\tLoss 0.8569 (0.9618)\tIOU 0.719 (0.494)\nEpoch: [1][160/200]\tLoss 0.9717 (0.9094)\tIOU 0.434 (0.499)\nEpoch: [1][170/200]\tLoss 0.8758 (0.9197)\tIOU 0.484 (0.497)\nEpoch: [1][180/200]\tLoss 0.8270 (0.9224)\tIOU 0.497 (0.473)\nEpoch: [1][190/200]\tLoss 1.0191 (0.9513)\tIOU 0.391 (0.429)\nEpoch: [1][Validation]\tVal_Loss: 1.01670\tVal_IOU: 0.33041\nEpoch: [2][0/200]\tLoss 0.7257 (0.7257)\tIOU 0.438 (0.438)\nEpoch: [2][10/200]\tLoss 1.0262 (0.8920)\tIOU 0.428 (0.412)\nEpoch: [2][20/200]\tLoss 0.9144 (0.9010)\tIOU 0.416 (0.450)\nEpoch: [2][30/200]\tLoss 0.7895 (0.9020)\tIOU 0.466 (0.458)\nEpoch: [2][40/200]\tLoss 0.7945 (0.8874)\tIOU 0.481 (0.367)\nEpoch: [2][50/200]\tLoss 1.1174 (0.9899)\tIOU 0.441 (0.357)\nEpoch: [2][60/200]\tLoss 0.9506 (0.9843)\tIOU 0.347 (0.393)\nEpoch: [2][70/200]\tLoss 1.0044 (0.9039)\tIOU 0.391 (0.451)\nEpoch: [2][80/200]\tLoss 0.9935 (0.8871)\tIOU 0.469 (0.537)\nEpoch: [2][90/200]\tLoss 0.7729 (0.9850)\tIOU 0.669 (0.408)\nEpoch: [2][100/200]\tLoss 0.8867 (0.8611)\tIOU 0.488 (0.510)\nEpoch: [2][110/200]\tLoss 0.8055 (0.9067)\tIOU 0.616 (0.435)\nEpoch: [2][120/200]\tLoss 0.8483 (0.8880)\tIOU 0.434 (0.388)\nEpoch: [2][130/200]\tLoss 0.7977 (0.8802)\tIOU 0.394 (0.414)\nEpoch: [2][140/200]\tLoss 0.8956 (0.8899)\tIOU 0.509 (0.470)\nEpoch: [2][150/200]\tLoss 1.0278 (0.9254)\tIOU 0.384 (0.375)\nEpoch: [2][160/200]\tLoss 1.0845 (0.8812)\tIOU 0.116 (0.399)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Process Process-44:\nProcess Process-43:\nProcess Process-41:\nProcess Process-42:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Ctrl+C, saving snapshot\ndone.\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n    if not self._poll(timeout):\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\n  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\nKeyboardInterrupt\nKeyboardInterrupt\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa9017958f34c2ebe48db883c714954206cee6f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}